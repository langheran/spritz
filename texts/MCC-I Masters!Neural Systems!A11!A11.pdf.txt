Assignment 11
Very Deep Convolutional Networks for Large-scale Image Recognition - VGG-16-19 Net
Nisim Hurst
Sunday 8 April 2018
Abstract
The paper on [1] by Karen Simonyan and Andrew Zisserman, describes and proves more efficient
ways of using small receptive field convolutional filters on ConvNets. First, to augment non-linearities
between layers, it uses cheap 1x1 convolutional filters. Then, it uses 3x3 filters to produce an equivalent
7x7 filter that also preserve information better. It also gradually test increasing network depths to derive
an optimal architecture.

Hypothesis
A reduction in the perception field produces more discriminative functions by summarizing less contiguous
pixels. However, a trade-off to keep in mind is the increased complexity of the output layer from the
convolutional layers chunk. So, reducing perceptive field will be possible only by reducing the ensued
complexity to a tractable level through other strategies. Increased depth, i.e. more layers, will produce equally
capable non-linear nets but with less parameters by a factor of O(n2 ).

Evidence and Results
Contribution
The main contribution of this paper is that it proposes an evolution of AlexNet architecture that allows
a more efficient use of more depth in convolutional layers . The authors found a correspondence between
a network that has a single 7x7 perception fields with 3 convolutional layers of 3x3 perception field. This
correspondence is less complex but equally capable of capturing non-linearities.
By using this architecture, the proposed set of networks won the first and second place on the ImageNet
Challenge 2014 (ILSVRC).
Architecture
1. 224x224 input layer (normalized around zero).
2. 5 distinct nets with:
1. 3x3 stack of convolutional layers on different lengths.
2. Stride fixed at 1.
3. Same padding.
4. Max-pooling layers of 2x2 and stride of 2.
5. The number of channels is doubled on each of the five nets.
6. One of them has 1x1 convolutional filters.
3. Three fully connected layers, the first two of 4096 units and the last one is a softmax of 1000 units
(classes).
4. Each hidden layers uses ReLU and one of the networks Local Response Normalization (LRN).

1

Implementation
1. Caffe C++ based.
2. 4 GTX Titan Black GPU's, 2-3 weeks of training.

Weaknesses
Height and weight of the image at each layer goes down as you go deeper in the network, until reaching 7x7.
Conversely, the number of channel increases doubles until reaching 512.
The authors state that

Future Work
Depth in convolutional networks is beneficial for classification accuracy.

References
[1] K. Simonyan and A. Zisserman, "Very Deep Convolutional Networks for Large-Scale Image Recognition,"
pp. 1­14, 2014.

2

