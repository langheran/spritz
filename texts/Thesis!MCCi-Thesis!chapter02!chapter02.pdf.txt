Chapter 2
1 Theoretical Background
In this chapter we review the existing state-of-the-art for solving the problem of segmenting a surveillance camera image from existing satellite images. With this research, we then propose a methodology to follow.
1.1 Problem characterization
First, recall that our objective is to segment the surveillance camera perspective. This task alone has the following characteristics:

 Different known-entities' distribution, i.e. at times we find lots of parked cars but out of season we find empty parking lots.

 Frequent occlusions like parked cars or pedestrians. 
 Changing climate conditions. 
 Cameras of different calibration parameters and color resolutions inside the
same parking lot.
Now, lets compare it with the characteristics of satellite image segmentation:

 Parking lots viewed from the space. 
 Texture appearance prevails over other features. 
 The view is orthogonal. 
 Instances can be rotated and form a manifold with the sole condition they
do not overlap. 
 Variable angle relative to the road, no more than 180 degrees. 
 Satellite images rarely have shadows and we could remove them by using
histogram normalization.
The main problem with the first setting mentioned at the beginning is the variability. To overcome this problems we will translate the results obtained in the satellite image perspective into the surveillance camera perspective. This problem is characterized by:

 2 images of different perspectives. One for the satellite image and one for
the surveillance system perspective. 
 Each image contains a set of features that have a non-null intersection. This
hypothesis is based on the fact that both are views of the same scene. Even though a particular method for identifying those features might produce a different set of disjunct sets, a human would still be able to identify them correctly. 
 Each image can have different color depths and palettes. However, the pixel intensities proportion in each channel can be similar to a certain degree. 
 The pixel resolution of the satellite image is fixed to a default zoom/scale (meters per pixel) when extracting it of the Geographic Information System (GIS) system. Usually just one satellite image is needed per parking lot. However, in the case of a dataset in which we have many parking lots, each one can have different dimensions, i.e. proportion between width and height. 
 The pixel resolution of the surveillance camera images varies depending on the cameras available in the surveillance system. It also varies regarding perspective. Depending on y position, a pixel can represent a different meter scale. However, surveillance systems usually provide standard image sizes (width and height), e.g. 800x600.
1.2 Sub-problem taxonomy definition
Recall that our problem is to segment parking blocks of arbitrary shapes while providing resiliency to partial occlusions. As far as this study goes, we haven't found any research that could achieve this task. However, a natural breakdown structure of our problem into easier-to-solve subproblems is hereby proposed. Two1 works of Young-Woo Seo relate the parking block segmentation task with the parking block detection task. First, [1] starts by defining a parking block:
We define a parking block as a row of parking spots all oriented in the same direction. Each parking block is characterized by the distance between neighboring parking spots in the block (i.e., "D1" in figure 1). Parking blocks are related to each other by two distance measures: the distance between conjugate parkingspots (i.e., "D2") and the distance between blocks (i.e., "D3" in the figure 1).
We will use a broader definition that includes also partially occluded parking spots. Also, we include parking blocks of arbitrary shapes onto splines and containing parking spots of different sizes or neighborhood arrangements.
1Their second paper from the same year just uses their previous results to construct a road network information between two points and included comparison with a Bayesian learning model. We will talk about this work in due course.
Thus, for parking block detection we will use the same taxonomy proposed by [2]. There, a clear division between parked vehicle detection and free parking spot detection is used to classify the approaches to drive parking block detection. We will further generalize into a higher level generalization by dividing between approaches from the camera perspective and from an aerial perspective. Consequently, we propose the following taxonomy:
1. Parking Block Segmentation from the Camera Perspective. Here, direct approaches to solving the parking block segmentation or parking spot detection problem are described.
2. Parking Block Segmentation from Aerial Images. Having reviewed the direct approaches, we will look indirect approaches at the aerial domain. We will see how those approaches have lower complexity and higher accuracy bounds. 1. Vehicle Detection to Segment Parking Blocks. Detecting cars is a new possibility ensuing from the change in perspective, nearly inexistent from the camera perspective. Here, we will review works that attempt to use detected cars to parking block detection. 2. Single Parking Spot Detection to Segment Parking Blocks. Most of the methods used in segmenting individual parking
blocks on the camera perspective can be used as well on the aerial top-view perspective. 3. Translation from Aerial Segmentation to Camera Segmentation. Finally, we will see the distortions and complexity introduced by translating the solution from one domain to the other.
1.2.1 Parking Block Segmentation from the Camera Perspective
When segmenting the camera perspective image in search of parking block, there are many conditions that can make this task harder. [3] classify them in 3 main sources:
1. Typical Attributes. Painted lines, other vehicles, kerbstones, non-plain floor, etc.
2. Road Conditions. Partially damaged parking lines, paving, etc. 3. Surrounding Conditions. Weather, illumination, light source angle,
etc.
We didn't found any approach in the literature that detects in the camera view parking blocks from detecting parked vehicles . Conversely, there are many papers that use individual parking spots detection to do so and will be brought
into question further on.
The most prominent feature of parking spots viewed from the camera perspective is given by the parking lines. So it is natural that early approaches involve segmenting the parking spots by color areas. Edge detection using a Kirsch filter over color intensities was used in [3]. There, the authors attempted to detect a parking spot behind a vehicle from its rear camera. As aforementioned, the authors do a great job at characterizing the problem and classify the parking spots into vertical and horizontal.
The results are not conclusive, though. Just four image show the qualitative result by means of overlays, rather than presenting a quantitative complete statistical proof. Also, a range on the size of a parking spot is assumed. Thus, height and width are bounded by the authors common sense and perception. Finally, the proposed method fails to deal with partial occlusions.
One of the most emblematic works in the literature was done in a series of three papers wrote by Huang et al. [2], [4] and [5]. In their work they strive to segment pixels of a static image through a 3-layer hierarchical Bayesian network method that project 3-dimensional cubes over the individual parking spaces per each row. These 3D projections helps the method overcome environmental and car occlusions.
Also, they use color to occupancy detection as proposed by [6].
Even though the work of Huang segment areas that belong to parking block, it can not be called dense-prediction. Given that they estimate global parameters and make global assumptions about the canonical parking spot inside the parking lot, they just estimate those parameters without flexibility for the individual cases. Thus, the authors doesn't provide results for the pixel by pixel segmentation, neither the dataset they used to reproduce their results. They provide results regarding the vacant parking spot detection and a nice survey of the literature on [7].
Another problem is that the method is dependent on the camera projection parameters. Also a row by row approach is taken to build single-row rectangular parking blocks and parking spots. Also, the authors assumed that occupancy status is independent from one parking spot to the other. This is rarely the case, because normally people park near the building entrance.
There are several observations worth noting. First, none of the approaches were able to overcome occlusions without first making assumptions over the global structure of the parking spots. Second, the methods don't used a direct supervised learning approach to segment the parking spaces. Now that we have seen how difficult it is for all the previous approaches to overcome occlusions lets see how changing the perspective of the scene change the toolbox to tackle the parking spot detection and parking block segmentation problems.
1.2.2 Parking Block Segmentation from Aerial Images
In contrast to the parking block segmentation from the Camera view domain, from aerial images we found several papers that do approximately the same. We found methods that detect parking blocks of arbitrary proportions though restricted to a rectangular shape. We also found segmentation pixel by pixel of complete parking blocks through human fine tuning. In this section we will briefly review those works in the context of the taxonomy proposed by [2], i.e. making the distinction between vehicle detection oriented approaches and parking spot detection oriented approaches.
1.2.2.1 Single Parking Spot Detection to Segment Parking Blocks
One of the forerunners on this domain was the work of [8]. The task performed was detecting parking spots and recovery of the ground texture using completeness
and correctness on a single parking spot. Neither learning or training was used.
Their method consist of the following. First, an elevation map produced from two images (stereo) is used to first remove vehicles from the ground. Then, a system for automatic surface texture and microstructure extraction (STME) is used to identify individual free parking spots. Also, a oriented region growing algorithm combined with some weight tuning formula is used to extract the parking spots from the previously extracted vehicles. Finally using the combined texture, the method repairs low quality or occluded parking spots.
A highly relevant feature of this work is that they transformed their results into a simulation system for the camera view perspective.
A work that is worth mentioning is [9]. The task performed was aerial patch imagery classification using the top 1 error rate and overall accuracy. The main focus of this paper is to compare a CNN with transfer learning based on the AlexNet network and a CNN with weights initialized from scratch.
The images were obtained from the New York State GIS Clearinghouse. 12,000 image patches of 60x60 pixels with an overlapping of 30 pixels, i.e. there were just 8,000 or less completely different patches to classify. This patch size limits the features that can be learned in the convolutional layers due to the imposed
padding.
Even though no segmentation was done, we can clearly see that parking lot (patches) can be accurately (94.3% overall accuracy) identified from aerial images using CNNs.
Another work for detecting single parking spots in aerial images is the work of [10]. Their proposal detects individual parking spot using custom correctness and completeness metrics. A second task was determining the parking lot structure in the form of global parameters, i.e. width, length and angle.
Their method applies Hough transform twice after having determined the principle orientations and using this result to estimate global parameters for the size and angle of the parking spots. Then they compare these results to the case just using the Hough transform.
They achieved 100% completeness and correctness using an aerial image covering 1,500m x 850m with a resolution 5cm per pixel that includes 3 parking lots.
Nevertheless, these completeness and correctness metrics were defined by the authors using a custom formula without giving any intuition neither on the literature nor stemming from the nature of the problem. Furthermore, the threshold
in this metric is assumed and no experimentation was done for calculating it.
1.2.2.2 Vehicle Detection to Segment Parking Blocks
Vehicle detection is suggested as future work by the work of [10] to aid parking block detection. Thus, the work of [11] detect rectangular parking lot areas from aerial images using vehicle and parking spot detection simultaneously.
The metric they used for measuring performance is correctness., i.e. the ratio between the correctly extracted parking lots and the total extracted parking lots. They also use a percent of detection metric, equal to the ratio between parking spots correctly extracted and the total real parking spots.
Given that the features to recognize a vehicle usually variate with each model, vehicle detection is usually a supervised task. Thus, for vehicle detection 2520 positive image patches of 14x32 pixels were used. Likewise, 4800 negative patches were used. For parking space detection a single image patch of 15x28 pixels was used. For parking lot detection, 4 images of 2000x2000 pixels were used. Zoom level is 20cm per pixel. The images were used to train a Haar-like detector in conjunction with an AdaBoost ensemble classifier. They achieved 66.9% completeness detecting vehicles, 30.2% completeness for parking spot detection and correctness of 95% in
rectangular parking lot extraction.
However, the task relies on several unrealistic assumptions. All the parking lots are assumed to be rectangular. Furthermore, a single angle of 90 degrees between the parking spot and the parking row is assumed.
There is also the work of [12]. The task performed was parking lot pixel segmentation using merging of parking spots and parked vehicles. The metric used was true positive rate (hit rate) over a single parking lot.
Recall that previously we asserted that vehicle detection was usually a supervised or self-supervised task. Well, the work of Mexas is a remarkable exception. They used morphological operations (tuned by a human) to detect the parked vehicles and didn't use any learning. The work is limited to a single Brazilian parking lot image was used. It has a resolution of 15 cm per pixel and size of 1000 x 1000 pixels.
The paper obviate comparison by variating the parameter values. Thus, we can assure without loss of precision, that they used human intuition alone. Therefore, the results are not statistically guarantied to be optimal. Finally, it is important to remark that the task performed was parking lot segmentation, in contrast to our task that is parking block segmentation.
1.2.3 Translation from Aerial Segmentation to Camera Segmentation
Translating the results between aerial and ground images is a well studied task. [13] showed how to estimate the camera extrinsic and intrinsic parameters and to enrich an aerial image from the ground view.
Solving the segmentation and detections problems in a top-view aerial image is simpler than solving it in the camera view. [14] applied an homography to transform the image into an aerial view and then applied the Hough transform to detect parking spot lines.
However, usually the image quality is not good after inverse perspective matching. So [15] developed a Top-View Transformation Model (TVTM) to generate a better images for parking spot detection. Luckily, nowadays GIS systems provide a second image of the same outdoors scene that obviate that requirement.
The work of [16] suggested that a rectification of the camera perspective into an aerial image using an homography can be used for prediction in a supervised task. Under that top-view image, they were able to identify if the parking spot is occupied or not using a SVM over texture feature vectors. Thus, they used a
simple symbolic approach based on homomorphisms to solve the linear system of equations given by 4 correspondence points. Those points were given by a human.
It is worth mentioning that [17] showed that it is possible to learn these correspondence points by using metric learning.
Given that the translation from aerial segmentation to camera segmentation is a well studied task, we conclude there is a high chance of achieving good results from translating the domain into the aerial (satellite) image.
1.3 Convolutional Neural Networks
Recall that on the first chapter we mentioned that Convolutional Neural Network (CNN)s were used successfully to identify buildings in satellite images. In this section, we explain a little bit what a CNN is, the basic inner-workings and some ancillary techniques for optimizing time and loss.
A perceptron network is a graphical structure that uses weights for describing the relation of each input unit with each output unit of two distinct feedforward layers. Thus, they are called fully connected. The current notion of a neural networks is what we commonly known as Multilayer Perceptron (MLP) [18]. The
MLP was born from single layer perceptrons [19]. By using the backpropagation algorithm scientists were able to add hidden layers [20]. Hinton et al. applied the backpropagation algorithm and other techniques for propagating the gradient to allow deeper layer levels. MLP's can be both learn their weights (parameters) by discriminating a given label (discriminatively) or by estimating joint probability distribution functions sensitive to context and previous knowledge (generatively). Discriminative learning models approximate the probability of a class label l given the data, p(l|d) . Conversely, generative learning models try to approximate the likelihood of a distribution given the label p(d|l). The supervised approach is the most common and it is discriminative. However, when dealing with images, just using densely connected layers is almost always too cumbersome. They fail to efficiently take advantage of the manifolds that are formed by a pattern of pixels in a spatial locality.
CNNs are one of the most popular neural network architecture applied to computer vision. They are specially useful when dealing with static images of variable size. In contrast with traditional networks, they use a small kernel aimed at detecting edges. The motivation of this kind of network is to learn a reduced set of shared parameters. Convolutional networks were around since the eighties and they had ever excelled in dealing with spatial features. For example LeNet-5 [21] by Yan
LeCun used this kind of architecture back in the 90's but they were considered shallow. Then, CNNs had a second burst of interest when AlexNet [22] won the ImageNet contest by more than 10% in 2012 by adding more layers with ReLU, max pooling and dropout regularization.
Figure 1: Steps performed by convolutional blocks.
Given that we are forcing the network to only learn a reduced filter the biasvariance tradeoff is pushed towards less variance. However, this limitation is balanced by the form in which we apply convolution over the image. The discrete convolution and also the cross-correlation operations 2 use a weighting function based on the position of the measurement relative to a locality window. Here are the main benefits of a convolutional neural network, as enumerated in [23]:
2the convolution without flipping the kernel
1. Sparse interactions. By making the kernel smaller than the input, convolution layers interact with a larger part of the input. Thus, the number of computations is reduced.
2. Parameter sharing. The same parameter is learned considering different inputs. Given than k is insignificant compared to inputs and outputs statistical efficiency and memory requirements are improved. This one and the previously enumerated feature are very desirable, considering the great size some parking lot images have.
3. Equivariant representation. The strict definition is given by f (g(x)) = g(f (x)). This means that the same representations are mantained on the output even if we move an object in the input. So, we can seek for the edges that are common in an object, or example those from texture. Finally, a parking spot is detected on an image, no matter if it is located on a corner of the image or at its center.
However, convolutional layers are sensitive to rotation and scaling. By using small pooling filters texture detection is made invariant for regions covered by the filter. The most common pooling layer is the max-pooling which just returns the maximum cell value in the convolution filter.1.4 Stochastic Gradient Descent
Learning an automatic set of features to describe a parking block can be a slow process. There are many variant camber, light conditions, occluding objects, etc. When having such variability in the examples, the Stochastic Gradient Descent (SGD) algorithm has been proved useful in many optimization problems, including CNN. The idea is to follow the gradient direction, one sample at a time, to find the function minima. Algorithm 1 shows the general pseudocode [24]:
Algorithm 1 Stochastic gradient descent Input: data - N sample vectors of D features to fit,  - random weights,  -
minibatch size
Output: Optimized weights 
1: repeat
2: Randomly permute data;
3: for i = 1 : N do
4: g = f (, zi)
The gradient operator given by the first moment of sample zi.
  proj ( - g)5:  Project the new weight vector by substracting a proportional quantity to the gradient
direction.
6: Update  7: end for 8: until converged
Assign  the length of the next batch size.
Usually we seek to minimize the root mean squared error. Machine learning algorithms that use this algorithm often stumble with local minima or saddle points show in Figure 2. Those are produced because a local minima in one dimension can be a maximum on another.
Figure 2: Saddle points are a usual problem in algorithms that use the SGD.
However, [25] empirically showed that neural networks are naturally invulnerable to this problem. Furthermore, CNNs usually optimize the algorithm by using:

 Momentum. A factor to increasingly diminish the weight changes as the algorithm progresses.

 Weight decay. Shrinks the weight coefficients towards zero as the algorithm advances. It serves to penalize overly complex models and avoid overfitting.
1.5 Affine Normalization
Affine normalization is a function between affine spaces that preserves points, straight lines and planes (a linear operator). All these elements are found in the
parallelograms that compose a parking block. In a neural network, we use it to restore the representational power with two vectors of trainable parameters, one that is a coefficient of the input vector and one independent for a bias. That is, we use a fully connected layer to learn non-linear combinations of the features, usually the ones that came from convolutional layers.
1.6 Residual Neural Units
In the U.S., a parking spot is defined by the ADA Accessibility Guidelines as a parallelogram composed of lines. It has been proven by [26] that simple edges like these lines are learned in the first layers of a CNN3. However, when the computational graphs becomes extremely deep, the application of the same weight parameters is equivalent to multiplying the matrix to the power of t . By eigendecomposition, if the matrix is squared, diagonalizable and normal then the matrix function becomes:
These results were inspired by the cat brain [27].
(1)
For small eigenvalues, the weights matrix W in equation 1 becomes nilpotent, for large eigenvalues, it explodes. This problem is called the vanishing and exploding gradients. Vanishing gradients makes impossible to tell were the weights should be heading to improve the cost function. Exploding gradients makes learning unstable.
Residual blocks were presented on [28] to circumvent these problems. It did so well on solving the vanishing gradient problem that it won the CVPR 2016 best paper. Residual neural networks seek to reformulate the layers with a reference to the layers input. That makes the identity function easier to learn equally well through out all the layers. So, the weights can easily become zero and allow the identity function to be passed to the next layer as needed:
(2)
Equation 2 shows the activation function, as depicted by Andrew Ng [29] , to the next layer. a stands for activation, g activation function, w weights and b bias term. The super indexes denote layer number. We can see that it is possible to
learn bias and weight equal to zero and just pass the activation of the previous layer to the next layer.
1.7 Homography Estimation
We previously mentioned that we need to translate the results from the satellite segmentation to the camera segmentation. An homography is a reprojection of the natural geometry of an image into another plane defined by four points. Simply put, it is a linear operator to translate between the two vector spaces. In computational terms, an homography is that matrix that will help us make the linear transformation.
The homography can be trivially calculated from the camera setup and making some assumptions, as we briefly describe in chapter 3. However, for homography estimation first we need the point correspondences. Given two images, these correspondences can be found automatically by following these steps:
1. Find key points such that {ui  Image1} and {ui  Image2} . There are many algorithms to find key points, e.g. using Harry's corner detector [30].
2. Represent key points by suitable descriptors. 3. Determine correspondences between ui  ui by matching descriptors 4. Prune wrong correspondences manually or using a threshold over distance
between the descriptors.
The homography can then be computed by solving equation 3:
OpenCV uses the linear least squares method by default for estimating the homography in equation 4 given any number of points:
Equation 5 shows the algebraic error that we usually want to minimize. There is also a set of geometric errors that can by used, e.g. the reprojection error [31].
OpenCV also has the option to use Random Sample Consensus (RANSAC):
1. Select four feature pairs (at random). 2. Compute homography H. 3. Compute inliers where SSD(pi, H, pi) < 4. 4. Keep largest set of inliers. 5. Re-compute least-squares H estimate on all the inliers.
There is a final method available in OpenCV parameters that is worth mentioning, namely the Least Median of Squares (LMeDS). The idea behind this method is to minimize the error with respect to the median instead of the mean. Again, just the first method was used. However, this brief detour helped us to see what options for calculating the homography we have.