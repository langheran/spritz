Eigenvalues and eigenvectors
In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by only a scalar
factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v. This condition can be written
as the equation
where  is a scalar in the field F, known as the eigenvalue, characteristic value, or characteristic root associated with the eigenvector v.
If the vector space V is finite-dimensional, then the linear transformation T can be represented as a square matrix A, and the vector v by a
column vector, rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the column vector on the right hand side in the equation
There is a direct correspondence between n-by-n square matrices and linear transformations from an n-dimensional vector space to itself, given any basis of the vector space. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.[1][2]
Geometrically, an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed.[3]
Contents
Overview History Eigenvalues and eigenvectors of matrices
Eigenvalues and the characteristic polynomial Algebraic multiplicity Eigenspaces, geometric multiplicity, and the eigenbasis for matrices Additional properties of eigenvalues Left and right eigenvectors Diagonalization and the eigendecomposition Variational characterization Matrix examples
Two-dimensional matrix example Three-dimensional matrix example Three-dimensional matrix example with complex eigenvalues Diagonal matrix example Triangular matrix example Matrix with repeated eigenvalues example Eigenvalues and eigenfunctions of differential operators Derivative operator example General definition Eigenspaces, geometric multiplicity, and the eigenbasis Zero vector as an eigenvector Spectral theory Associative algebras and representation theory Dynamic equations Calculation Classical method Eigenvalues Eigenvectors
Simple iterative methods Modern methods

Modern methods Applications
Eigenvalues of geometric transformations Schrödinger equation Molecular orbitals Geology and glaciology Principal component analysis Vibration analysis Eigenfaces Tensor of moment of inertia Stress tensor Graphs Basic reproduction number See also Footnotes Notes References External links Theory Demonstration applets
Overview
Eigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix eigen- is adopted from the German word eigen for "proper", "characteristic".[4] Originally utilized to study principal axes of the rotational motion of rigid bodies, eigenvalues and eigenvectors have a wide range of applications, for example in stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization.
In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value , called an eigenvalue. This condition can be written as the equation

referred to as the eigenvalue equation or eigenequation. In general,  may be any scalar. For example,  may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or complex.

The Mona Lisa example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a shear mapping. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points along the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.
Linear transformations can take many different forms, mapping vectors in a variety of vector spaces, so the eigenvectors can also take many forms. For example, the linear transformation could be a differential operator like , in which case the eigenvectors are functions called eigenfunctions that are scaled by that differential operator, such as

In this shear mapping the red arrow changes direction but the blue arrow does not. The blue arrow is an eigenvector of this shear mapping because it does not change direction, and since its length is unchanged, its eigenvalue is 1.

Alternatively, the linear transformation could take the form of an n by n matrix, in which case the eigenvectors are n by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an n by n matrix A, then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplication

where the eigenvector v is an n by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to decompose the matrix, for example by diagonalizing it.
Eigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix eigen- is applied liberally when naming them:
The set of all eigenvectors of a linear transformation, each paired with its corresponding eigenvalue, is called the eigensystem of that transformation.[5][6] The set of all eigenvectors of T corresponding to the same eigenvalue, together with the zero vector, is called an eigenspace or characteristic space of T.[7][8] If the set of eigenvectors of T form a basis of the domain of T, then this basis is called an eigenbasis.
History
Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.
In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes.[9] Lagrange realized that the principal axes are the eigenvectors of the inertia matrix.[10] In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions.[11] Cauchy also coined the term racine caractéristique (characteristic root) for what is now called eigenvalue; his term survives in characteristic equation.[12][13]
Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book Théorie analytique de la chaleur.[14] Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.[11] This was extended by Hermite in 1855 to what are now called Hermitian matrices.[12] Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle,[11] and Clebsch found the corresponding result for skew-symmetric matrices.[12] Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.[11]
In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called Sturm­Liouville theory.[15] Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincaré studied Poisson's equation a few years later.[16]
At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices.[17] He was the first to use the German word eigen, which means "own", to denote eigenvalues and eigenvectors in 1904,[18] though he may have been following a related usage by Helmholtz. For some time, the standard term in English was "proper value", but the more distinctive term "eigenvalue" is standard today.[19]
The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis[20] and Vera Kublanovskaya[21] in 1961.[22]
Eigenvalues and eigenvectors of matrices
Eigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.[23][24] Furthermore, linear transformations can be represented using matrices,[1][2] which is especially common in numerical and computational applications.[25]
Consider n-dimensional vectors that are formed as a list of n scalars, such as the three-dimensional vectors
These vectors are said to be scalar multiples of each other, or parallel or collinear, if there is a scalar  such that
In this case  = -1/20.
Now consider the linear transformation of n-dimensional vectors defined by an n by n matrix A,

or

where, for each row,

.

If it occurs that v and w are scalar multiples, that is if

Matrix A acts by stretching the vector x, not changing its direction, so x is an eigenvector of A.

(1)
then v is an eigenvector of the linear transformation A and the scale factor  is the eigenvalue corresponding to that eigenvector. Equation (1) is the eigenvalue equation for the matrix A.
Equation (1) can be stated equivalently as
(2)
where I is the n by n identity matrix.
Eigenvalues and the characteristic polynomial
Equation (2) has a non-zero solution v if and only if the determinant of the matrix (A - I) is zero. Therefore, the eigenvalues of A are values of  that satisfy the equation
(3)
Using Leibniz' rule for the determinant, the left hand side of Equation (3) is a polynomial function of the variable  and the degree of this polynomial is n, the order of the matrix A. Its coefficients depend on the entries of A, except that its term of degree n is always (-1)nn. This polynomial is called the characteristic polynomial of A. Equation (3) is called the characteristic equation or the secular equation of A. The fundamental theorem of algebra implies that the characteristic polynomial of an n-by-n matrix A, being a polynomial of degree n, can be factored into the product of n linear terms,
(4)
where each i may be real but in general is a complex number. The numbers 1, 2, ... n, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of A. As a brief example, which is described in more detail in the examples section later, consider the matrix

Taking the determinant of (M - I), the characteristic polynomial of M is

Setting the characteristic polynomial equal to zero, it has roots at  = 1 and  = 3, which are the two eigenvalues of M. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of v in the equation Mv = v. In this example, the eigenvectors are any non-zero scalar multiples of

If the entries of the matrix A are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be irrational numbers even if all the entries of A are rational numbers or even if they are all integers. However, if the entries of A are all algebraic numbers, which include the rationals, the eigenvalues are complex algebraic numbers.
The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugates, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.
Algebraic multiplicity
Let i be an eigenvalue of an n by n matrix A. The algebraic multiplicity A(i) of the eigenvalue is its multiplicity as a root of the characteristic polynomial, that is, the largest integer k such that ( - i)k divides evenly that polynomial.[8][26][27] Suppose a matrix A has dimension n and d  n distinct eigenvalues. Whereas Equation (4) factors the characteristic polynomial of A into the product of n linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of d terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,
If d = n then the right hand side is the product of n linear terms and this is the same as Equation (4). The size of each eigenvalue's algebraic multiplicity is related to the dimension n as
If A(i) = 1, then i is said to be a simple eigenvalue.[27] If A(i) equals the geometric multiplicity of i, A(i), defined in the next section, then i is said to be a semisimple eigenvalue.
Eigenspaces, geometric multiplicity, and the eigenbasis for matrices
Given a particular eigenvalue  of the n by n matrix A, define the set E to be all vectors v that satisfy Equation (2),
On one hand, this set is precisely the kernel or nullspace of the matrix (A - I). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of A associated with . So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with , and E equals the nullspace of (A - I). E is called the eigenspace or characteristic space of A associated with .[7][8] In general  is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace is that it is a linear subspace, so E is a linear subspace of n. Because the eigenspace E is a linear subspace, it is closed under addition. That is, if two vectors u and v belong to the set E, written (u,v)  E, then (u + v)  E or equivalently A(u + v) = (u + v). This can be checked using the distributive property of matrix multiplication. Similarly, because E is a linear subspace, it is closed under scalar multiplication. That is, if v  E and  is a complex number, (v)  E or equivalently A(v) = (v). This can be checked by noting that multiplication of complex matrices by complex numbers is commutative. As long as u + v and v are not zero, they are also eigenvectors of A associated with .
The dimension of the eigenspace E associated with , or equivalently the maximum number of linearly independent eigenvectors associated with , is referred to as the eigenvalue's geometric multiplicity A(). Because E is also the nullspace of (A - I), the geometric multiplicity of  is the dimension of the nullspace of (A - I), also called the nullity of (A - I), which relates to the dimension and rank of (A - I) as
Because of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n.

To prove the inequality

, consider how the definition of geometric multiplicity implies the existence of orthonormal eigenvectors

, such that

. We can therefore find a (unitary) matrix whose first columns are these eigenvectors, and whose remaining

columns can be any orthonormal set of

vectors orthogonal to these eigenvectors of . Then has full rank and is therefore invertible, and

with a matrix whose top left block is the diagonal matrix . This implies that

. In other words, is similar to ,

which implies that

. But from the definition of we know that

contains a factor

, which means that the algebraic

multiplicity of must satisfy

.

Suppose A has d  n distinct eigenvalues 1, 2, ..., d, where the geometric multiplicity of i is A(i). The total geometric multiplicity of A,

is the dimension of the union of all the eigenspaces of A's eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of A. If A = n, then
The direct sum of the eigenspaces of all of A's eigenvalues is the entire vector space n A basis of n can be formed from n linearly independent eigenvectors of A; such a basis is called an eigenbasis Any vector in n can be written as a linear combination of eigenvectors of A
Additional properties of eigenvalues
Let A be an arbitrary n by n matrix of complex numbers with eigenvalues 1, 2, ..., n. Each eigenvalue appears A(i) times in this list, where A(i) is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:
The trace of A, defined as the sum of its diagonal elements, is also the sum of all eigenvalues,
[28][29][30]
The determinant of A is the product of all its eigenvalues,
[28][31][32]
The eigenvalues of the kth power of A, i.e. the eigenvalues of Ak, for any positive integer k, are 1k, 2k, ..., nk. The matrix A is invertible if and only if every eigenvalue is nonzero. If A is invertible, then the eigenvalues of A-1 are 1/1, 1/2, ..., 1/n and each eigenvalue's geometric multiplicity coincides. Moreover, since the characteristic polynomial of the inverse is the reciprocal polynomial of the original, the eigenvalues share the same algebraic multiplicity. If A is equal to its conjugate transpose A*, or equivalently if A is Hermitian, then every eigenvalue is real. The same is true of any symmetric real matrix. If A is not only Hermitian but also positive-definite, positive-semidefinite, negative-definite, or negative-semidefinite, then every eigenvalue is positive, non-negative, negative, or non-positive, respectively. If A is unitary, every eigenvalue has absolute value |i| = 1.
Left and right eigenvectors
Many disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word "eigenvector" in the context of matrices almost always refers to a right eigenvector, namely a column vector that right multiplies the n by n matrix A in the defining equation, Equation (1),
The eigenvalue and eigenvector problem can also be defined for row vectors that left multiply matrix A. In this formulation, the defining equation is
where  is a scalar and u is a 1 by n matrix. Any row vector u satisfying this equation is called a left eigenvector of A and  is its associated eigenvalue. Taking the transpose of this equation,

Comparing this equation to Equation (1), it follows immediately that a left eigenvector of A is the same as the transpose of a right eigenvector of AT, with the same eigenvalue. Furthermore, since the characteristic polynomial of AT is the same as the characteristic polynomial of A, the eigenvalues of the left eigenvectors of A are the same as the eigenvalues of the right eigenvectors of AT.
Diagonalization and the eigendecomposition
Suppose the eigenvectors of A form a basis, or equivalently A has n linearly independent eigenvectors v1, v2, ..., vn with associated eigenvalues 1, 2, ..., n. The eigenvalues need not be distinct. Define a square matrix Q whose columns are the n linearly independent eigenvectors of A,
Since each column of Q is an eigenvector of A, right multiplying A by Q scales each column of Q by its associated eigenvalue,

With this in mind, define a diagonal matrix  where each diagonal element ii is the eigenvalue associated with the ith column of Q. Then Because the columns of Q are linearly independent, Q is invertible. Right multiplying both sides of the equation by Q-1, or by instead left multiplying both sides by Q-1,

A can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition and it is a similarity transformation. Such a matrix A is said to be similar to the diagonal matrix  or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and  represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as .
Conversely, suppose a matrix A is diagonalizable. Let P be a non-singular square matrix such that P-1AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P must therefore be an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis if and only if A is diagonalizable.
A matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.

Variational characterization

In the Hermitian case, eigenvalues can be given a variational characterization. The largest eigenvalue of is the maximum value of the

quadratic form

. A value of that realizes that maximum, is an eigenvector.

Matrix examples

Two-dimensional matrix example Consider the matrix

The figure on the right shows the effect of this transformation on point coordinates in the plane. The eigenvectors v of this transformation satisfy Equation (1), and the values of  for which the determinant of the matrix (A - I) equals zero are the eigenvalues.
Taking the determinant to find characteristic polynomial of A,

Setting the characteristic polynomial equal to zero, it has roots at  = 1 and  = 3, which are the two eigenvalues of A. For  = 1, Equation (2) becomes,

Any non-zero vector with v1 = -v2 solves this equation. Therefore,
is an eigenvector of A corresponding to  = 1, as is any scalar multiple of this vector. For  = 3, Equation (2) becomes

Any non-zero vector with v1 = v2 solves this equation. Therefore,

is an eigenvector of A corresponding to  = 3, as is any scalar multiple of this vector.
Thus, the vectors v=1 and v=3 are eigenvectors of A associated with the eigenvalues  = 1 and  = 3, respectively.

Three-dimensional matrix example Consider the matrix
The characteristic polynomial of A is

The roots of the characteristic polynomial are 2, 1, and 11, which are the only three

eigenvalues of A. These eigenvalues correspond to the eigenvectors

and

, or any non-zero multiple thereof.

The transformation matrix A = preserves the direction of vectors parallel to v=1 = [1 -1]T (in purple) and v=3 = [1 1]T (in blue). The vectors in red are not parallel to either eigenvector, so, their directions are changed by the transformation. The blue vectors after the transformation are three times the length of the original (their eigenvalue is 3), while the lengths of the purple vectors are unchanged (reflecting an eigenvalue of 1). See also: An extended version, showing all four quadrants.

Three-dimensional matrix example with complex eigenvalues Consider the cyclic permutation matrix

This matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1 - 3, whose roots are

where i = is the imaginary unit. For the real eigenvalue 1 = 1, any vector with three equal non-zero entries is an eigenvector. For example,
For the complex conjugate pair of imaginary eigenvalues, note that

T hen

and

Therefore, the other two eigenvectors of A are complex and are two complex eigenvectors also appear in a complex conjugate pair,

and

with eigenvalues 2 and 3, respectively. Note that the

Diagonal matrix example
Matrices with entries only along the main diagonal are called diagonal matrices. The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrix

The characteristic polynomial of A is
which has the roots 1 = 1, 2 = 2, and 3 = 3. These roots are the diagonal elements as well as the eigenvalues of A. Each diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,

respectively, as well as scalar multiples of these vectors.
Triangular matrix example A matrix whose elements above the main diagonal are all zero is called a lower triangular matrix, while a matrix whose elements below the main diagonal are all zero is called an upper triangular matrix. As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.
Consider the lower triangular matrix,

The characteristic polynomial of A is
which has the roots 1 = 1, 2 = 2, and 3 = 3. These roots are the diagonal elements as well as the eigenvalues of A. These eigenvalues correspond to the eigenvectors,

respectively, as well as scalar multiples of these vectors.
Matrix with repeated eigenvalues example As in the previous example, the lower triangular matrix

has a characteristic polynomial that is the product of its diagonal elements,
The roots of this polynomial, and hence the eigenvalues, are 2 and 3. The algebraic multiplicity of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is A = 4 = n, the order of the characteristic polynomial and the dimension of A. On the other hand, the geometric multiplicity of the eigenvalue 2 is only 1, because its eigenspace is spanned by just one vector [0 1 -1 1]T and is therefore 1-dimensional. Similarly, the geometric multiplicity of the eigenvalue 3 is 1 because its eigenspace is spanned by just one vector [0 0 0 1]T. The total geometric multiplicity A is 2, which is the smallest it could be for a matrix with two distinct eigenvalues. Geometric multiplicities are defined in a later section.
Eigenvalues and eigenfunctions of differential operators
The definitions of eigenvalue and eigenvectors of a linear transformation T remains valid even if the underlying vector space is an infinitedimensional Hilbert or Banach space. A widely used class of linear transformations acting on infinite-dimensional spaces are the differential operators on function spaces. Let D be a linear differential operator on the space C of infinitely differentiable real functions of a real argument t. The eigenvalue equation for D is the differential equation
The functions that satisfy this equation are eigenvectors of D and are commonly called eigenfunctions.
Derivative operator example
Consider the derivative operator with eigenvalue equation
This differential equation can be solved by multiplying both sides by dt/f(t) and integrating. Its solution, the exponential function
is the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for  = 0 the eigenfunction f(t) is a constant. The main eigenfunction article gives other examples.
General definition
The concept of eigenvalues and eigenvectors extends naturally to arbitrary linear transformations on arbitrary vector spaces. Let V be any vector space over some field K of scalars, and let T be a linear transformation mapping V into V,
We say that a non-zero vector v  V is an eigenvector of T if and only if there exists a scalar   K such that
(5)
This equation is called the eigenvalue equation for T, and the scalar  is the eigenvalue of T corresponding to the eigenvector v. Note that T(v) is the result of applying the transformation T to the vector v, while v is the product of the scalar  with v.[33]
Eigenspaces, geometric multiplicity, and the eigenbasis
Given an eigenvalue , consider the set

which is the union of the zero vector with the set of all eigenvectors associated with . E is called the eigenspace or characteristic space of T associated with .
By definition of a linear transformation,
for (x,y)  V and   K. Therefore, if u and v are eigenvectors of T associated with eigenvalue , namely (u,v)  E, then
So, both u + v and v are either zero or eigenvectors of T associated with , namely (u+v,v)  E, and E is closed under addition and scalar multiplication. The eigenspace E associated with  is therefore a linear subspace of V.[8][34][35] If that subspace has dimension 1, it is sometimes called an eigenline.[36]
The geometric multiplicity T() of an eigenvalue  is the dimension of the eigenspace associated with , i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.[8][27] By the definition of eigenvalues and eigenvectors, T()  1 because every eigenvalue has at least one eigenvector.
The eigenspaces of T always form a direct sum. As a consequence, eigenvectors of different eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension n of the vector space on which T operates, and there cannot be more than n distinct eigenvalues.[37]
Any subspace spanned by eigenvectors of T is an invariant subspace of T, and the restriction of T to such a subspace is diagonalizable. Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is diagonalizable.
Zero vector as an eigenvector
While the definition of an eigenvector used in this article excludes the zero vector, it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.[38]
Consider again the eigenvalue equation, Equation (5). Define an eigenvalue to be any scalar   K such that there exists a non-zero vector v  V satisfying Equation (5). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in K to be an eigenvalue. Define an eigenvector v associated with the eigenvalue  to be any vector that, given , satisfies Equation (5). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation (5), so the zero vector is included among the eigenvectors by this alternate definition.
Spectral theory
If  is an eigenvalue of T, then the operator (T - I) is not one-to-one, and therefore its inverse (T - I)-1 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (T - I) may not have an inverse even if  is not an eigenvalue.
For this reason, in functional analysis eigenvalues can be generalized to the spectrum of a linear operator T as the set of all scalars  for which the operator (T - I) has no bounded inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.
Associative algebras and representation theory
One can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an algebra representation ­ an associative algebra acting on a module. The study of such actions is the field of representation theory.
T he representation-theoretical concept of weight is an analog of eigenvalues, while weight vectors and weight spaces are the analogs of eigenvectors and eigenspaces, respectively.
Dynamic equations
The simplest difference equations have the form

The solution of this equation for x in terms of t is found by using its characteristic equation

which can be found by stacking into matrix form a set of equations consisting of the above difference equation and the k ­ 1 equations

giving a k-dimensional system of the first order in the stacked variable vector

in terms of its once-lagged value,

and taking the characteristic equation of this system's matrix. This equation gives k characteristic roots

for use in the solution equation

A similar procedure is used for solving a differential equation of the form

Calculation
The calculation of eigenvalues and eigenvectors is a topic where theory, as presented in elementary linear algebra textbooks, is often very far from practice.

Classical method
The classical method is to first find the eigenvalues, and then calculate the eigenvectors for each eigenvalue. It is in several ways poorly suited for non-exact arithmetics such as floating-point.

Eigenvalues
The eigenvalues of a matrix can be determined by finding the roots of the characteristic polynomial. This is easy for difficulty increases rapidly with the size of the matrix.

matrices, but the

In theory, the coefficients of the characteristic polynomial can be computed exactly, since they are sums of products of matrix elements; and there are algorithms that can find all the roots of a polynomial of arbitrary degree to any required accuracy.[39] However, this approach is not
viable in practice because the coefficients would be contaminated by unavoidable round-off errors, and the roots of a polynomial can be an extremely sensitive function of the coefficients (as exemplified by Wilkinson's polynomial).[39] Even for matrices whose elements are integers
the calculation becomes nontrivial, because the sums are very long; the constant term is the determinant, which for an is a sum of different products.[note 1]

Explicit algebraic formulas for the roots of a polynomial exist only if the degree is 4 or less. According to the Abel­Ruffini theorem there is no general, explicit and exact algebraic formula for the roots of a polynomial with degree 5 or more. (Generality matters because any polynomial with degree is the characteristic polynomial of some companion matrix of order .) Therefore, for matrices of order 5 or more, the eigenvalues and eigenvectors cannot be obtained by an explicit algebraic formula, and must therefore be computed by approximate numerical methods. Even the exact formula for the roots of a degree 3 polynomial is numerically impractical.

Eigenvectors
Once the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrix

we can find its eigenvectors by solving the equation , that is

This matrix equation is equivalent to two linear equations
that is
Both equations reduce to the single linear equation . Therefore, any vector of the form , for any non-zero real number , is an eigenvector of with eigenvalue . The matrix above has another eigenvalue . A similar calculation shows that the corresponding eigenvectors are the non-zero solutions of

, that is, any vector of the form , for any non-zero real number .

Simple iterative methods

The converse approach, of first seeking the eigenvectors and then determining each eigenvalue from its eigenvector, turns out to be far more

tractable for computers. The easiest algorithm here consists of picking an arbitrary starting vector and then repeatedly multiplying it with the

matrix (optionally normalising the vector to keep its elements of reasonable size); surprisingly this makes the vector converge towards an

eigenvector. A variation is to instead multiply the vector by

; this causes it to converge to an eigenvector of the eigenvalue closest to .

If is (a good approximation of) an eigenvector of , then the corresponding eigenvalue can be computed as

where denotes the conjugate transpose of .
Modern methods
Efficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961. [39] Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm. For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.[39]
Most numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation, although sometimes the implementors choose to discard the eigenvector information as soon as it is not needed anymore.
Applications

Eigenvalues of geometric transformations
The following table presents some example transformations in the plane along with their 2×2 matrices, eigenvalues, and eigenvectors.

scaling

unequal scaling rotation horizontal shear hyperbolic rotation

illustration

matrix

characteristic polynomial eigenvalues algebraic multipl.
geometric multipl.

,

eigenvectors All non-zero vectors

Note that the characteristic equation for a rotation is a quadratic equation with discriminant

, which is a negative number whenever 

is not an integer multiple of 180°. Therefore, except for these special cases, the two eigenvalues are complex numbers,

; and all

eigenvectors have non-real entries. Indeed, except for those special cases, a rotation changes the direction of every nonzero vector in the plane.

A linear transformation that takes a square to a rectangle of the same area (a squeeze mapping) has reciprocal eigenvalues.

Schrödinger equation
An example of an eigenvalue equation where the transformation is represented in terms of a differential operator is the time-independent Schrödinger equation in quantum mechanics:

where , the Hamiltonian, is a second-order differential operator and , the wavefunction, is one of its eigenfunctions corresponding to the eigenvalue , interpreted as its energy.

However, in the case where one is interested only in the bound state solutions of the Schrödinger equation, one looks for within the space of square integrable functions. Since this space is a Hilbert space with a well-defined scalar product, one can introduce a basis set in which and can be represented as a one-dimensional array (i.e., a vector) and a matrix respectively. This allows one to represent the Schrödinger equation in a matrix form.
The bra­ket notation is often used in this context. A vector, which represents a state of the system, in the Hilbert space of square integrable functions is represented by . In this notation, the Schrödinger equation is:
where is an eigenstate of and represents the eigenvalue. is an observable self adjoint operator, the infinite-dimensional analog of Hermitian matrices. As in the matrix case, in the equation above is understood to be the vector obtained by application of the transformation to .

The wavefunctions associated with the bound

states of an electron in a hydrogen atom can

be seen as the eigenvectors of the hydrogen

atom Hamiltonian as well as of the angular

momentum operator. They are associated with

eigenvalues interpreted as their energies

(increasing downward:

) and angular

momentum (increasing across: s, p, d, ...). The

illustration shows the square of the absolute

value of the wavefunctions. Brighter areas

correspond to higher probability density for a

position measurement. The center of each

figure is the atomic nucleus, a proton.

Molecular orbitals
In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree­Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree­Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations.

Geology and glaciology
In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,[40][41] or as a Stereonet on a Wulff Net.[42]

The output for the orientation tensor is in the three orthogonal (perpendicular) axes of space. The three eigenvectors are ordered by their

eigenv alues

;[43] then is the primary orientation/dip of clast, is the secondary and is the tertiary, in terms of strength. The clast

orientation is defined as the direction of the eigenvector, on a compass rose of 360°. Dip is measured as the eigenvalue, the modulus of the

tensor: this is valued from 0° (no dip) to 90° (vertical). The relative values of , , and are dictated by the nature of the sediment's fabric. If

, the fabric is said to be isotropic. If

, the fabric is said to be planar. If

, the fabric is said to be linear.[44]

Principal component analysis
T he eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability

among a number of observed data.
Principal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling.
Vibration analysis
Eigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed by

or

that is, acceleration is proportional to position (i.e., we expect to be sinusoidal in time). In dimensions, becomes a mass matrix and a stiffness matrix. Admissible solutions are then a linear combination of solutions to the generalized eigenvalue problem
where is the eigenvalue and is the (imaginary) angular frequency. Note that the principal vibration modes are different from the principal compliance modes, which are the eigenvectors of alone. Furthermore, damped vibration, governed by
leads to a so-called quadratic eigenvalue problem,

PCA of the multivariate Gaussian distribution centered at with a standard deviation of 3 in roughly the
direction and of 1 in the orthogonal direction. The vectors shown are unit eigenvectors of the (symmetric, positive-semidefinite) covariance matrix scaled by the square root of the corresponding eigenvalue. (Just as in the onedimensional case, the square root is taken because the standard deviation is more readily visualized than the variance.

This can be reduced to a generalized eigenvalue problem by algebraic manipulation at the cost of solving a larger system.
The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.

Eigenfaces

Mode Shape of a Tuning Fork at Eigenfrequency 440.09 Hz

In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel.[45] The dimension of this vector space is the number of pixels. The

eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of

principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial

recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related

to eigen vision systems determining hand gestures has also been made.

Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.

Tensor of moment of inertia
In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.

Stress tensor
In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.

Graphs

In spectral graph theory, an eigenvalue of a graph is defined as an eigenvalue of the graph's adjacency

matrix , or (increasingly) of the graph's Laplacian matrix due to its discrete Laplace operator, which is

either (sometimes called the combinatorial Laplacian) or

(sometimes called the

normalized Laplacian), where is a diagonal matrix with equal to the degree of vertex , and in ,

t he th diagonal entry is

. The th principal eigenvector of a graph is defined as either the

eigenvector corresponding to the th largest or th smallest eigenvalue of the Laplacian. The first

principal eigenvector of the graph is also referred to merely as the principal eigenvector.

Eigenfaces as examples of eigenvectors

The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering.

Basic reproduction number
The basic reproduction number ( ) is a fundamental number in the study of how infectious diseases spread. If one infectious person is put into a population of completely susceptible people, then is the average number of people that one typical infectious person will infect. The generation time of an infection is the time, , from one person becoming infected to the next person becoming infected. In a heterogeneous population, the next generation matrix defines how many people in the population will become infected after time has passed. is then the largest eigenvalue of the next generation matrix.[46][47]
See also
Antieigenvalue theory Eigenoperator Eigenplane Eigenvalue algorithm Introduction to eigenstates Jordan normal form List of numerical analysis software Nonlinear eigenproblem Quadratic eigenvalue problem Singular value
Footnotes
Notes
References
Akivis, Max A.; Goldberg, Vladislav V. (1969), Tensor calculus, Russian, Science Publishers, Moscow Aldrich, John (2006), "Eigenvalue, eigenfunction, eigenvector, and related terms" (http://jeff560.tripod.com/e.html), in Jeff Miller (Editor), Earliest Known Uses of Some of the Words of Mathematics (http://jeff560.tripod.com/e.html), retrieved 2006-08-22 Alexandrov, Pavel S. (1968), Lecture notes in analytical geometry, Russian, Science Publishers, Moscow Anton, Howard (1987), Elementary Linear Algebra (5th ed.), New York: Wiley, ISBN 0-471-84819-0 Beauregard, Raymond A.; Fraleigh, John B. (1973), A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields, Boston: Houghton Mifflin Co., ISBN 0-395-14017-X Beezer, Robert A. (2006), A first course in linear algebra (http://linear.ups.edu/), Free online book under GNU licence, University of Puget Sound

Betteridge, Harold T. (1965), The NewCassell's German Dictionary, New York: Funk & Wagnall, LCCN 58-7924 (https://lccn.loc.gov/58-7924 ) Bowen, Ray M.; Wang, Chao-Cheng (1980), Linear and multilinear algebra, Plenum Press, New York, ISBN 0-306-37508-7 Brown, Maureen (October 2004), Illuminating Patterns of Perception: An Overviewof Q Methodology Burden, Richard L.; Faires, J. Douglas (1993), Numerical Analysis (5th ed.), Boston: Prindle, Weber and Schmidt, ISBN 0-534-93219-3 Carter, Tamara A.; Tapia, Richard A.; Papaconstantinou, Anne, Linear Algebra: An Introduction to Linear Algebra for Pre-Calculus Students (http://ceee.rice.edu/Books/LA/index.html), Rice University, Online Edition, retrieved 2008-02-19 Cohen-Tannoudji, Claude (1977), "Chapter II. The mathematical tools of quantum mechanics", Quantum mechanics, John Wiley & Sons, ISBN 0-471-16432-1 Curtis, Charles W. (1999), Linear Algebra: An Introductory Approach (4th ed.), Springer, ISBN 0-387-90992-3 Demmel, James W. (1997), Applied numerical linear algebra, SIAM, ISBN 0-89871-389-7 Fraleigh, John B. (1976), A First Course In Abstract Algebra (2nd ed.), Reading: Addison-Wesley, ISBN 0-201-01984-1 Fraleigh, John B.; Beauregard, Raymond A. (1995), Linear algebra (3rd ed.), Addison-Wesley Publishing Company, ISBN 0-201-83999-7 Friedberg, Stephen H.; Insel, Arnold J.; Spence, Lawrence E. (1989), Linear algebra (2nd ed.), Englewood Cliffs, New Jersey 07632: Prentice Hall, ISBN 0-13-537102-3 Gelfand, I. M. (1971), Lecture notes in linear algebra, Russian, Science Publishers, Moscow Gohberg, Israel; Lancaster, Peter; Rodman, Leiba (2005), Indefinite linear algebra and applications, Basel-Boston-Berlin: Birkhäuser Verlag, ISBN 3-7643-7349-0 Golub, Gene F.; van der Vorst, Henk A. (2000), "Eigenvalue computation in the 20th century", Journal of Computational and Applied Mathematics, 123: 35­65, Bibcode:2000JCoAM.123...35G (http://adsabs.harvard.edu/abs/2000JCoAM.123...35G), doi:10.1016/S03770427(00)00413-1 (https://doi.org/10.1016%2FS0377-0427%2800%2900413-1) Golub, Gene H.; Van Loan, Charles F. (1996), Matrix computations (3rd ed.), Johns Hopkins University Press, Baltimore, Maryland, ISBN 9780-8018-5414-9 Greub, Werner H. (1975), Linear Algebra (4th ed.), Springer-Verlag, New York, ISBN 0-387-90110-8 Halmos, Paul R. (1987), Finite-dimensional vector spaces (8th ed.), New York: Springer-Verlag, ISBN 0-387-90093-4 Hawkins, T. (1975), "Cauchy and the spectral theory of matrices", Historia Mathematica, 2: 1­29, doi:10.1016/0315-0860(75)90032-4 (https:// doi.org/10.1016%2F0315-0860%2875%2990032-4) Hefferon, Jim (2001), Linear Algebra (http://joshua.smcvt.edu/linearalgebra/), Online book, St Michael's College, Colchester, Vermont, USA Herstein, I. N. (1964), Topics In Algebra, Waltham: Blaisdell Publishing Company, ISBN 978-1114541016 Horn, Roger A.; Johnson, Charles F. (1985), Matrix analysis, Cambridge University Press, ISBN 0-521-30586-1 Kline, Morris (1972), Mathematical thought from ancient to modern times, Oxford University Press, ISBN 0-19-501496-0 Korn, Granino A.; Korn, Theresa M. (2000), "Mathematical Handbook for Scientists and Engineers: Definitions, Theorems, and Formulas for Reference and Review", NewYork: McGraw-Hill (2nd Revised ed.), Dover Publications, Bibcode:1968mhse.book.....K (http://adsabs.harvard. edu/abs/1968mhse.book.....K), ISBN 0-486-41147-8 Kuttler, Kenneth (2007), An introduction to linear algebra (http://www.math.byu.edu/~klkuttle/Linearalgebra.pdf) (PDF), Online e-book in PDF format, Brigham Young University Lancaster, P. (1973), Matrix theory, Russian, Moscow, Russia: Science Publishers Larson, Ron; Edwards, Bruce H. (2003), Elementary linear algebra (5th ed.), Houghton Mifflin Company, ISBN 0-618-33567-6 Lipschutz, Seymour (1991), Schaum's outline of theory and problems of linear algebra, Schaum's outline series (2nd ed.), New York: McGraw-Hill Companies, ISBN 0-07-038007-4 Meyer, Carl D. (2000), Matrix analysis and applied linear algebra, Society for Industrial and Applied Mathematics (SIAM), Philadelphia, ISBN 978-0-89871-454-8 Nering, Evar D. (1970), Linear Algebra and Matrix Theory (2nd ed.), New York: Wiley, LCCN 76091646 (https://lccn.loc.gov/76091646) (in Russian)Pigolkina, T. S.; Shulman, V. S. (1977). "Eigenvalue". In Vinogradov, I. M. Mathematical Encyclopedia. 5. Moscow: Soviet Encyclopedia. Press, William H.; Teukolsky, Saul A.; Vetterling, William T.; Flannery, Brian P. (2007), Numerical Recipes: The Art of Scientific Computing (3rd ed.), ISBN 9780521880688 Roman, Steven (2008), Advanced linear algebra (3rd ed.), New York: Springer Science + Business Media, LLC, ISBN 978-0-387-72828-5 Sharipov, Ruslan A. (1996), Course of Linear Algebra and Multidimensional Geometry: the textbook, arXiv:math/0405323 (https://arxiv.org/a bs/math/0405323), Bibcode:2004math......5323S (http://adsabs.harvard.edu/abs/2004math......5323S), ISBN 5-7477-0099-5 Shilov, Georgi E. (1977), Linear algebra, Translated and edited by Richard A. Silverman, New York: Dover Publications, ISBN 0-486-63518-X Shores, Thomas S. (2007), Applied linear algebra and matrix analysis, Springer Science+Business Media, LLC, ISBN 0-387-33194-8 Strang, Gilbert (1993), Introduction to linear algebra, Wellesley-Cambridge Press, Wellesley, Massachusetts, ISBN 0-9614088-5-5 Strang, Gilbert (2006), Linear algebra and its applications, Thomson, Brooks/Cole, Belmont, California, ISBN 0-03-010567-6
External links
What are Eigen Values? (http://www.physlink.com/education/AskExperts/ae520.cfm) ­ non-technical introduction from PhysLink.com's "Ask the Experts"

Eigen Values and Eigen Vectors Numerical Examples (http://people.revoledu.com/kardi/tutorial/LinearAlgebra/EigenValueEigenVector.html) ­ Tutorial and Interactive Program from Revoledu. Introduction to Eigen Vectors and Eigen Values (https://web.archive.org/web/20100325112901/http://khanexercises.appspot.com/video?v=Ph fbEr2btGQ) ­ lecture from Khan Academy Hill, Roger (2009). " ­ Eigenvalues" (http://www.sixtysymbols.com/videos/eigenvalues.htm). Sixty Symbols. Brady Haran for the University of Nottingham. "A Beginner's Guide to Eigenvectors" (http://deeplearning4j.org/eigenvector). Deeplearning4j. 2015. Eigenvectors and eigenvalues | Essence of linear algebra, chapter 10 (https://www.youtube.com/watch?v=PFDu9oVAE-g&list=PLZHQObOW TQDPD3MizzM2xVFitgF8hE_ab&index=14) ­ A visual explanation with 3Blue1Brown
Theory
Hazewinkel, Michiel, ed. (2001) [1994], "Eigen value" (https://www.encyclopediaofmath.org/index.php?title=p/e035150), Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 Hazewinkel, Michiel, ed. (2001) [1994], "Eigen vector" (https://www.encyclopediaofmath.org/index.php?title=p/e035180), Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 "Eigenvalue (of a matrix)" (http://planetmath.org/?op=getobj&from=objects&id=4397). PlanetMath. Eigenvector (http://mathworld.wolfram.com/Eigenvector.html) ­ Wolfram MathWorld Eigen Vector Examination working applet (http://ocw.mit.edu/ans7870/18/18.06/javademo/Eigen/) Same Eigen Vector Examination as above in a Flash demo with sound (http://web.mit.edu/18.06/www/Demos/eigen-applet-all/eigen_sound_a ll.html) Computation of Eigenvalues (http://www.sosmath.com/matrix/eigen1/eigen1.html) Numerical solution of eigenvalue problems (http://www.cs.utk.edu/~dongarra/etemplates/index.html) Edited by Zhaojun Bai, James Demmel, Jack Dongarra, Axel Ruhe, and Henk van der Vorst Eigenvalues and Eigenvectors on the Ask Dr. Math forums: [1] (http://mathforum.org/library/drmath/view/55483.html), [2] (http://mathforum.org/li brary/drmath/view/51989.html)
Demonstration applets
Java applet about eigenvectors in the real plane (http://scienceapplets.blogspot.com/2012/03/eigenvalues-and-eigenvectors.html) Wolfram Language functionality for Eigenvalues, Eigenvectors and Eigensystems (http://reference.wolfram.com/language/guide/DifferentialEq uations.html)

Jordan matrix
In the mathematical discipline of matrix theory, a Jordan block over a ring (whose identities are the zero 0 and one 1) is a matrix composed of 0 elements everywhere except for the diagonal, which is filled with a fixed element , and for the superdiagonal, which is composed of ones. The concept is named after Camille Jordan.

Every Jordan block is thus specified by its dimension n and its eigenvalue and is indicated as . Any block diagonal matrix whose

blocks are Jordan blocks is called a Jordan matrix; using either the or the " " symbol, the

block diagonal square

matrix consisting of diagonal blocks, where the first is , the second is , , the -th is , can be compactly indicated as

or , respectively. For example the matrix

is a Jordan matrix with a block with eigenvalue , two blocks with eigenvalue the imaginary unit , and a block with eigenvalue 7. Its Jordan-block structure can also be written as either or .

Contents
Linear algebra Functions of matrices Dynamical systems Linear ordinary differential equations See also Notes References

Linear algebra
Any square matrix whose elements are in an algebraically closed field is similar to a Jordan matrix , also in , which is unique up to a permutation of its diagonal blocks themselves. is called the Jordan normal form of and corresponds to a generalization of the diagonalization procedure.[1][2][3] A diagonalizable matrix is similar, in fact, to a special case of Jordan matrix: the matrix whose blocks are all .[4][5][6]

More generally, given a Jordan matrix

, i.e. whose diagonal block,

is the Jordan block and whose

diagonal elements may not all be distinct, the geometric multiplicity of for the matrix , indicated as , corresponds to the

number of Jordan blocks whose eigenvalue is . Whereas the index of an eigenvalue for , indicated as , is defined as the dimension

of the largest Jordan block associated to that eigenvalue.

The same goes for all the matrices similar to , so can be defined accordingly with respect to the Jordan normal form of for any of its eigenvalues . In this case one can check that the index of for is equal to its multiplicity as a root of the minimal polynomial of (whereas, by definition, its algebraic multiplicity for , , is its multiplicity as a root of the characteristic polynomial of , i.e.

). An equivalent necessary and sufficient condition for to be diagonalizable in is that all of its eigenvalues have index equal to , i.e. its minimal polynomial has only simple roots.
Note that knowing a matrix's spectrum with all of its algebraic/geometric multiplicities and indexes does not always allow for the computation of its Jordan normal form (this may be a sufficient condition only for spectrally simple, usually low-dimensional matrices): t h e Jordan decomposition is, in general, a computationally challenging task. From the vector space point of view, the Jordan decomposition is equivalent to finding an orthogonal decomposition (i.e. via direct sums of eigenspaces represented by Jordan blocks) of the domain which the associated generalized eigenvectors make a basis for.

Functions of matrices

Let (i.e. a complex matrix) and

be the change of basis matrix to the Jordan normal form of , i.e.

. Now let be

a holomorphic function on an open set such that

, i.e. the spectrum of the matrix is contained inside the domain of holomorphy

of . Let

be the power series expansion of around

, which will be hereinafter supposed to be 0 for simplicity's sake. The matrix is

then defined via the following formal power series

is absolutely convergent with respect to the Euclidean norm of . To put it another way, converges absolutely for every square matrix whose spectral radius is less than the radius of convergence of around and is uniformly convergent on any compact subsets of
satisfying this property in the matrix Lie group topology.

The Jordan normal form allows the computation of functions of matrices without explicitly computing an infinite series, which is one of

the main achievements of Jordan matrices. Using the facts that the power ( ) of a diagonal block matrix is the diagonal block matrix

whose blocks are the powers of the respective blocks, i.e.

, and that

, the above matrix power series

becomes

where the last series must not be computed explicitly via power series of every Jordan block. In fact, if , any holomorphic function of a Jordan block is the following upper triangular matrix:

As a consequence of this, the computation of any functions of a matrix is straightforward whenever its Jordan normal form and its

change-of-basis matrix are known. Also,

, i.e. every eigenvalue

corresponds to the eigenvalue

, but it has, in

general, different algebraic multiplicity, geometric multiplicity and index. However, the algebraic multiplicity may be computed as

follows:

The function of a linear transformation between vector spaces can be defined in a similar way according to the holomorphic functional calculus, where Banach space and Riemann surface theories play a fundamental role. In the case of finite-dimensional spaces, both theories perfectly match.
Dynamical systems
Now suppose a (complex) dynamical system is simply defined by the equation

where

is the ( -dimensional) curve parametrization of an orbit on the Riemann surface of the dynamical system, whereas is

an complex matrix whose elements are complex functions of a -dimensional parameter . Even if

(i.e. continuously

depends on the parameter ) the Jordan normal form of the matrix is continuously deformed almost everywhere on but, in general,

not everywhere: there is some critical submanifold of on which the Jordan form abruptly changes its structure whenever the

parameter crosses or simply "travels" around it (monodromy). Such changes mean that several Jordan blocks (either belonging to

different eigenvalues or not) join together to a unique Jordan block, or vice versa (i.e. one Jordan block splits into two or more different

ones). Many aspects of bifurcation theory for both continuous and discrete dynamical systems can be interpreted with the analysis of

functional Jordan matrices.

From the tangent space dynamics, this means that the orthogonal decomposition of the dynamical system's phase space changes and, for example, different orbits gain periodicity, or lose it, or shift from a certain kind of periodicity to another (such as period-doubling, cfr. logistic map).

In a sentence, the qualitative behaviour of such a dynamical system may substantially change as the versal deformation of the Jordan normal form of .

Linear ordinary differential equations
The simplest example of a dynamical system is a system of linear, constant-coefficient, ordinary differential equations, i.e. let :

and

whose direct closed-form solution involves computation of the matrix exponential:

Another way, provided the solution is restricted to the local Lebesgue space of -dimensional vector fields

transform

. In this case

, is to use its Laplace

The matrix function

is called the resolvent matrix of the differential operator . It is meromorphic with respect to the complex

parameter since its matrix elements are rational functions whose denominator is equal for all to

. Its polar singularities are the

eigenvalues of , whose order equals their index for it, i.e.

.

See also
Jordan decomposition Jordan normal form Holomorphic functional calculus Matrix exponential Logarithm of a matrix Dynamical system Bifurcation theory State space (controls)

Notes

References
Beauregard, Raymond A.; Fraleigh, John B. (1973), A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields, Boston: Houghton Mifflin Co., ISBN 0-395-14017-X Golub, Gene H.; Van Loan, Charles F. (1996), Matrix Computations (3rd ed.), Baltimore: Johns Hopkins University Press, ISBN 08018-5414-8

Nering, Evar D. (1970), Linear Algebra and Matrix Theory (2nd ed.), New York: Wiley, LCCN 76091646 (https://lccn.loc.gov/76091646 )

Jordan normal form

In linear algebra, a Jordan normal form (often called Jordan canonical form)[1] of a linear operator on a finite-dimensional vector space is an upper triangular matrix of a particular form called a Jordan matrix, representing the operator with respect to some basis. Such a matrix has each non-zero off-diagonal entry equal to 1, immediately above the main diagonal (on the superdiagonal), and with identical diagonal entries to the left and below them.

Let V be a vector space over a field K. Then a basis with respect to which the matrix

has the required form exists if and only if all eigenvalues of the matrix lie in K, or

equivalently if the characteristic polynomial of the operator splits into linear factors

over K. This condition is always satisfied if K is algebraically closed (for instance, if it is

the field of complex numbers). The diagonal entries of the normal form are the

eigenvalues (of the operator), and the number of times each eigenvalue occurs is called the algebraic multiplicity of the eigenvalue.[2][3][4]

An example of a matrix in Jordan normal form. The grey blocks are called Jordan

blocks. If the operator is originally given by a square matrix M, then its Jordan normal form is

also called the Jordan normal form of M. Any square matrix has a Jordan normal form

if the field of coefficients is extended to one containing all the eigenvalues of the matrix. In spite of its name, the normal form for a given

M is not entirely unique, as it is a block diagonal matrix formed of Jordan blocks, the order of which is not fixed; it is conventional to

group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size.[2][3][4]

The Jordan­Chevalley decomposition is particularly simple with respect to a basis for which the operator takes its Jordan normal form. The diagonal form for diagonalizable matrices, for instance normal matrices, is a special case of the Jordan normal form.[5][6][7]
The Jordan normal form is named after Camille Jordan, who first stated the Jordan decomposition theorem in 1870.[8]

Contents
Overview Notation Motivation
Complex matrices Generalized eigenvectors A proof Uniqueness
Real matrices
Consequences Spectral mapping theorem Cayley­Hamilton theorem Minimal polynomial Invariant subspace decompositions
Generalizations Matrices with entries in a field Compact operators Holomorphic functional calculus The finite-dimensional case Poles of an operator
Example
Numerical analysis
Matrix functions

SNeoeteaslso References
Overview
Notation
Some textbooks have the ones on the subdiagonal, i.e., immediately below the main diagonal instead of on the superdiagonal. The eigenvalues are still on the main diagonal.[9][10]
Motivation
An n × n matrix A is diagonalizable if and only if the sum of the dimensions of the eigenspaces is n. Or, equivalently, if and only if A has n linearly independent eigenvectors. Not all matrices are diagonalizable. Consider the following matrix:
Including multiplicity, the eigenvalues of A are  = 1, 2, 4, 4. The dimension of the eigenspace corresponding to the eigenvalue 4 is 1 (and not 2), so A is not diagonalizable. However, there is an invertible matrix P such that A = PJP-1, where
The matrix J is almost diagonal. This is the Jordan normal form of A. The section Example below fills in the details of the computation.
Complex matrices
In general, a square complex matrix A is similar to a block diagonal matrix
where each block Ji is a square matrix of the form
So there exists an invertible matrix P such that P-1AP = J is such that the only non-zero entries of J are on the diagonal and the superdiagonal. J is called the Jordan normal form of A. Each Ji is called a Jordan block of A. In a given Jordan block, every entry on the superdiagonal is 1. Assuming this result, we can deduce the following properties:
Counting multiplicity, the eigenvalues of J, therefore A, are the diagonal entries. Given an eigenvalue i, its geometric multiplicity is the dimension of Ker(A - i I), and it is the number of Jordan blocks corresponding to i.[11] The sum of the sizes of all Jordan blocks corresponding to an eigenvalue i is its algebraic multiplicity.[11] A is diagonalizable if and only if, for every eigenvalue  of A, its geometric and algebraic multiplicities coincide. The Jordan block corresponding to  is of the form  I + N, where N is a nilpotent matrix defined as Nij = i,j-1 (where  is the Kronecker delta). The nilpotency of N can be exploited when calculating f(A) where f is a complex analytic function. For example, in principle the Jordan form could give a closed-form expression for the exponential exp(A). The number of Jordan blocks corresponding to  of size at least j is dim Ker(A - I)j - dim Ker(A - I)j-1. Thus, the number of Jordan blocks of size exactly j is

Given an eigenvalue i, its multiplicity in the minimal polynomial is the size of its largest Jordan block.
Generalized eigenvectors
Consider the matrix A from the example in the previous section. The Jordan normal form is obtained by some similarity transformation P-1AP = J, i.e.,
Let P have column vectors pi, i = 1, ..., 4, then
We see that

For i = 1,2,3 we have gives

, i.e., pi is an eigenvector of A corresponding to the eigenvalue i. For i=4, multiplying both sides by

But , so

T hus,
Vectors such as are called generalized eigenvectors of A.
Thus, given an eigenvalue , its corresponding Jordan block gives rise to a Jordan chain. The generator, or lead vector, say pr, of the chain is a generalized eigenvector such that (A -  I)rpr = 0, where r is the size of the Jordan block. The vector p1 = (A -  I)r-1pr is an eigenvector corresponding to . In general, pi is a preimage of pi-1 under A -  I. So the lead vector generates the chain via multiplication by (A -  I).[12][13]
Therefore, the statement that every square matrix A can be put in Jordan normal form is equivalent to the claim that there exists a basis consisting only of eigenvectors and generalized eigenvectors of A.
A proof
We give a proof by induction that any complex-valued matrix A may be put in Jordan normal form. The 1 × 1 case is trivial. Let A be an n × n matrix. Take any eigenvalue  of A. The range of A -  I, denoted by Ran(A -  I), is an invariant subspace of A. Also, since  is an eigenvalue of A, the dimension of Ran(A -  I), r, is strictly less than n. Let A' denote the restriction of A to Ran(A -  I), By inductive hypothesis, there exists a basis {p1, ..., pr} such that A' , expressed with respect to this basis, is in Jordan normal form.
Next consider the subspace Ker(A -  I). If

the desired result follows immediately from the rank­nullity theorem. This would be the case, for example, if A was Hermitian. Otherwise, if

let the dimension of Q be s  r. Each vector in Q is an eigenvector of A' corresponding to eigenvalue . So the Jordan form of A ' must

contain s Jordan chains corresponding to s linearly independent eigenvectors. So the basis {p1, ..., pr} must contain s vectors, say {pr-s+1, ..., pr}, that are lead vectors in these Jordan chains from the Jordan normal form of A'. We can "extend the chains" by taking the preimages of these lead vectors. (This is the key step of argument; in general, generalized eigenvectors need not lie in Ran(A -  I).) Let qi be such that
Clearly no non-trivial linear combination of the qi can lie in Ker(A -  I). Furthermore, no non-trivial linear combination of the qi can be in Ran(A -  I), for that would contradict the assumption that each pi is a lead vector in a Jordan chain. The set {qi}, being preimages of the linearly independent set {pi} under A -  I, is also linearly independent. Finally, we can pick any linearly independent set {z1, ..., zt} that spans
By construction, the union of the three sets {p1, ..., pr}, {qr-s +1, ..., qr}, and {z1, ..., zt} is linearly independent. Each vector in the union is either an eigenvector or a generalized eigenvector of A. Finally, by the rank­nullity theorem, the cardinality of the union is n. In other words, we have found a basis that consists of eigenvectors and generalized eigenvectors of A, and this shows A can be put in Jordan normal form.
Uniqueness
It can be shown that the Jordan normal form of a given matrix A is unique up to the order of the Jordan blocks.
Knowing the algebraic and geometric multiplicities of the eigenvalues is not sufficient to determine the Jordan normal form of A. Assuming the algebraic multiplicity m() of an eigenvalue  is known, the structure of the Jordan form can be ascertained by analyzing the ranks of the powers (A -  I)m(). To see this, suppose an n × n matrix A has only one eigenvalue . So m() = n. The smallest integer k1 such that
is the size of the largest Jordan block in the Jordan form of A. (This number k1 is also called the index of . See discussion in a following section.) The rank of

is the number of Jordan blocks of size k1. Similarly, the rank of

is twice the number of Jordan blocks of size k1 plus the number of Jordan blocks of size k1-1. The general case is similar.
This can be used to show the uniqueness of the Jordan form. Let J1 and J2 be two Jordan normal forms of A. Then J1 and J2 are similar and have the same spectrum, including algebraic multiplicities of the eigenvalues. The procedure outlined in the previous paragraph can be used to determine the structure of these matrices. Since the rank of a matrix is preserved by similarity transformation, there is a bijection between the Jordan blocks of J1 and J2. This proves the uniqueness part of the statement.

Real matrices

I f A is a real matrix, its Jordan form can still be non-real. Instead of representing it with complex eigenvalues and 1's on the superdiagonal, as discussed above, there exists a real invertible matrix P such that P-1AP = J is a real block diagonal matrix with each

block being a real Jordan block. A real Jordan block is either identical to a complex Jordan block (if the corresponding eigenvalue is

real), or is a block matrix itself, consisting of 2×2 blocks (for non-real eigenvalue

with given algebraic multiplicity) of the form

and describe multiplication by in the complex plane. The superdiagonal blocks are 2×2 identity matrices and hence in this representation the matrix dimensions are larger than the complex Jordan form. The full real Jordan block is given by

This real Jordan form is a consequence of the complex Jordan form. For a real matrix the nonreal eigenvectors and generalized eigenvectors can always be chosen to form complex conjugate pairs. Taking the real and imaginary part (linear combination of the vector and its conjugate), the matrix has this form with respect to the new basis.
Consequences
One can see that the Jordan normal form is essentially a classification result for square matrices, and as such several important results from linear algebra can be viewed as its consequences.
Spectral mapping theorem
Using the Jordan normal form, direct calculation gives a spectral mapping theorem for the polynomial functional calculus: Let A be an n × n matrix with eigenvalues 1, ..., n, then for any polynomial p, p(A) has eigenvalues p(1), ..., p(n).
Cayley­Hamilton theorem
The Cayley­Hamilton theorem asserts that every matrix A satisfies its characteristic equation: if p is the characteristic polynomial of A, then p(A) = 0. This can be shown via direct calculation in the Jordan form, since any Jordan block for  is annihilated by (X - )m where m is the multiplicity of the root  of p, the sum of the sizes of the Jordan blocks for , and therefore no less than the size of the block in
question. The Jordan form can be assumed to exist over a field extending the base field of the matrix, for instance over the splitting field
of p; this field extension does not change the matrix p(A) in any way.
Minimal polynomial
The minimal polynomial P of a square matrix A is the unique monic polynomial of least degree, m, such that P(A) = 0. Alternatively, the set of polynomials that annihilate a given A form an ideal I in C[x], the principal ideal domain of polynomials with complex coefficients. The monic element that generates I is precisely P.
Let 1, ..., q be the distinct eigenvalues of A, and si be the size of the largest Jordan block corresponding to i. It is clear from the
Jordan normal form that the minimal polynomial of A has degree si.
While the Jordan normal form determines the minimal polynomial, the converse is not true. This leads to the notion of elementary divisors. The elementary divisors of a square matrix A are the characteristic polynomials of its Jordan blocks. The factors of the minimal polynomial m are the elementary divisors of the largest degree corresponding to distinct eigenvalues.
The degree of an elementary divisor is the size of the corresponding Jordan block, therefore the dimension of the corresponding invariant subspace. If all elementary divisors are linear, A is diagonalizable.
Invariant subspace decompositions
The Jordan form of a n × n matrix A is block diagonal, and therefore gives a decomposition of the n dimensional Euclidean space into invariant subspaces of A. Every Jordan block Ji corresponds to an invariant subspace Xi. Symbolically, we put
where each Xi is the span of the corresponding Jordan chain, and k is the number of Jordan chains. One can also obtain a slightly different decomposition via the Jordan form. Given an eigenvalue i, the size of its largest corresponding Jordan block si is called the index of i and denoted by (i). (Therefore, the degree of the minimal polynomial is the sum of all indices.) Define a subspace Yi by
This gives the decomposition

where l is the number of distinct eigenvalues of A. Intuitively, we glob together the Jordan block invariant subspaces corresponding to the same eigenvalue. In the extreme case where A is a multiple of the identity matrix we have k = n and l = 1.
The projection onto Yi and along all the other Yj ( j  i ) is called the spectral projection of A at i and is usually denoted by P(i ; A). Spectral projections are mutually orthogonal in the sense that P(i ; A) P(j ; A) = 0 if i  j. Also they commute with A and their sum is the identity matrix. Replacing every i in the Jordan matrix J by one and zeroising all other entries gives P(i ; J), moreover if U J U-1 is the similarity transformation such that A = U J U-1 then P(i ; A) = U P(i ; J) U-1. They are not confined to finite dimensions. See below for their application to compact operators, and in holomorphic functional calculus for a more general discussion.
Comparing the two decompositions, notice that, in general, l  k. When A is normal, the subspaces Xi's in the first decomposition are one-dimensional and mutually orthogonal. This is the spectral theorem for normal operators. The second decomposition generalizes more easily for general compact operators on Banach spaces.
It might be of interest here to note some properties of the index, (). More generally, for a complex number , its index can be defined as the least non-negative integer () such that
So () > 0 if and only if  is an eigenvalue of A. In the finite-dimensional case, ()  the algebraic multiplicity of .
Generalizations
Matrices with entries in a field
Jordan reduction can be extended to any square matrix M whose entries lie in a field K. The result states that any M can be written as a sum D + N where D is semisimple, N is nilpotent, and DN = ND. This is called the Jordan­Chevalley decomposition. Whenever K contains the eigenvalues of M, in particular when K is algebraically closed, the normal form can be expressed explicitly as the direct sum of Jordan blocks. Similar to the case when K is the complex numbers, knowing the dimensions of the kernels of (M - I)k for 1  k  m, where m is the algebraic multiplicity of the eigenvalue , allows one to determine the Jordan form of M. We may view the underlying vector space V as a K[x]-module by regarding the action of x on V as application of M and extending by K-linearity. Then the polynomials (x - )k are the elementary divisors of M, and the Jordan normal form is concerned with representing M in terms of blocks associated to the elementary divisors.
The proof of the Jordan normal form is usually carried out as an application to the ring K[x] of the structure theorem for finitely generated modules over a principal ideal domain, of which it is a corollary.
Compact operators
In a different direction, for compact operators on a Banach space, a result analogous to the Jordan normal form holds. One restricts to compact operators because every point x in the spectrum of a compact operator T, the only exception being when x is the limit point of the spectrum, is an eigenvalue. This is not true for bounded operators in general. To give some idea of this generalization, we first reformulate the Jordan decomposition in the language of functional analysis.
Holomorphic functional calculus Let X be a Banach space, L(X) be the bounded operators on X, and (T) denote the spectrum of T  L(X). The holomorphic functional calculus is defined as follows:
Fix a bounded operator T. Consider the family Hol(T) of complex functions that is holomorphic on some open set G containing (T). Let  = {i} be a finite collection of Jordan curves such that (T) lies in the inside of , we define f(T) by
The open set G could vary with f and need not be connected. The integral is defined as the limit of the Riemann sums, as in the scalar

case. Although the integral makes sense for continuous f, we restrict to holomorphic functions to apply the machinery from classical function theory (e.g., the Cauchy integral formula). The assumption that (T) lie in the inside of  ensures f(T) is well defined; it does not depend on the choice of . The functional calculus is the mapping  from Hol(T) to L(X) given by
We will require the following properties of this functional calculus: 1.  extends the polynomial functional calculus. 2. The spectral mapping theorem holds: (f(T)) = f((T)). 3.  is an algebra homomorphism.
The finite-dimensional case In the finite-dimensional case, (T) = {i} is a finite discrete set in the complex plane. Let ei be the function that is 1 in some open neighborhood of i and 0 elsewhere. By property 3 of the functional calculus, the operator
is a projection. Moreoever, let i be the index of i and
The spectral mapping theorem tells us
has spectrum {0}. By property 1, f(T) can be directly computed in the Jordan form, and by inspection, we see that the operator f(T)ei(T) is the zero matrix. By property 3, f(T) ei(T) = ei(T) f(T). So ei(T) is precisely the projection onto the subspace
The relation
implies
where the index i runs through the distinct eigenvalues of T. This is exactly the invariant subspace decomposition
given in a previous section. Each ei(T) is the projection onto the subspace spanned by the Jordan chains corresponding to i and along the subspaces spanned by the Jordan chains corresponding to j for j  i. In other words, ei(T) = P(i;T). This explicit identification of the operators ei(T) in turn gives an explicit form of holomorphic functional calculus for matrices:
For all f  Hol(T),
Notice that the expression of f(T) is a finite sum because, on each neighborhood of i, we have chosen the Taylor series expansion of f centered at i. Poles of an operator Let T be a bounded operator  be an isolated point of (T). (As stated above, when T is compact, every point in its spectrum is an isolated point, except possibly the limit point 0.)

The point  is called a pole of operator T with order  if the resolvent function RT defined by
has a pole of order  at . We will show that, in the finite-dimensional case, the order of an eigenvalue coincides with its index. The result also holds for compact operators. Consider the annular region A centered at the eigenvalue  with sufficiently small radius  such that the intersection of the open disc B() and (T) is {}. The resolvent function RT is holomorphic on A. Extending a result from classical function theory, RT has a Laurent series representation on A:

where

and C is a small circle centered at .

By the previous discussion on the functional calculus,
where is 1 on and 0 elsewhere.
But we have shown that the smallest positive integer m such that
and
is precisely the index of , (). In other words, the function RT has a pole of order () at .

Example
This example shows how to calculate the Jordan normal form of a given matrix. As the next section explains, it is important to do the computation exactly instead of rounding the results.
Consider the matrix

which is mentioned in the beginning of the article.
The characteristic polynomial of A is
This shows that the eigenvalues are 1, 2, 4 and 4, according to algebraic multiplicity. The eigenspace corresponding to the eigenvalue 1 can be found by solving the equation Av =  v. It is spanned by the column vector v = (-1, 1, 0, 0)T. Similarly, the eigenspace corresponding to the eigenvalue 2 is spanned by w = (1, -1, 0, 1)T. Finally, the eigenspace corresponding to the eigenvalue 4 is also onedimensional (even though this is a double eigenvalue) and is spanned by x = (1, 0, -1, 1)T. So, the geometric multiplicity (i.e., the dimension of the eigenspace of the given eigenvalue) of each of the three eigenvalues is one. Therefore, the two eigenvalues equal to 4 correspond to a single Jordan block, and the Jordan normal form of the matrix A is the direct sum

There are three chains. Two have length one: {v} and {w}, corresponding to the eigenvalues 1 and 2, respectively. There is one chain of length two corresponding to the eigenvalue 4. To find this chain, calculate

where I is the 4 x 4 identity matrix. Pick a vector in the above span that is not in the kernel of A - 4I, e.g., y = (1,0,0,0)T. Now, (A - 4I)y = x and (A - 4I)x = 0, so {y, x} is a chain of length two corresponding to the eigenvalue 4. The transition matrix P such that P-1AP = J is formed by putting these vectors next to each other as follows
A computation shows that the equation P-1AP = J indeed holds.
If we had interchanged the order of which the chain vectors appeared, that is, changing the order of v, w and {x, y} together, the Jordan blocks would be interchanged. However, the Jordan forms are equivalent Jordan forms.
Numerical analysis
If the matrix A has multiple eigenvalues, or is close to a matrix with multiple eigenvalues, then its Jordan normal form is very sensitive to perturbations. Consider for instance the matrix
If  = 0, then the Jordan normal form is simply
However, for   0, the Jordan normal form is
This ill conditioning makes it very hard to develop a robust numerical algorithm for the Jordan normal form, as the result depends critically on whether two eigenvalues are deemed to be equal. For this reason, the Jordan normal form is usually avoided in numerical analysis; the stable Schur decomposition[14] or pseudospectra[15] are better alternatives.
Matrix functions
The Jordan normal form is the most convenient for computation of the matrix functions (though it may be not the best choice for computer computations). Let f(z) be an analytical function of a complex argument. Applying the function on a n×n Jordan block J with eigenvalue  results in an upper triangular matrix:

so that the elements of the k-th superdiagonal of the resulting matrix are expression shall be applied to each Jordan block.
The following example shows the application to the power function f(z)=zn:

. For a matrix of general Jordan normal form the above

where the binomial coefficients are defined as

negative n the identity

may be of use.

See also
Canonical basis Canonical form Frobenius normal form Jordan matrix Jordan­Chevalley decomposition Matrix decomposition Modal matrix Weyr canonical form

Notes

. For integer positive n it reduces to standard definition of the coefficients. For

References
Beauregard, Raymond A.; Fraleigh, John B. (1973), A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields, Boston: Houghton Mifflin Co., ISBN 0-395-14017-X Bronson, Richard (1970), Matrix Methods: An Introduction, New York: Academic Press, LCCN 70097490 (https://lccn.loc.gov/700974 90) Cullen, Charles G. (1966), Matrices and Linear Transformations, Reading: Addison-Wesley, LCCN 66021267 (https://lccn.loc.gov/660 21267) Dunford, N.; Schwartz, J. T. (1958), Linear Operators, Part I: General Theory, Interscience Finkbeiner II, Daniel T. (1978), Introduction to Matrices and Linear Transformations (3rd ed.), W. H. Freeman and Company Franklin, Joel N. (1968), Matrix Theory, Englewood Cliffs: Prentice-Hall, LCCN 68016345 (https://lccn.loc.gov/68016345) Golub, Gene H.; Van Loan, Charles F. (1996), Matrix Computations (3rd ed.), Baltimore: Johns Hopkins University Press, ISBN 08018-5414-8 Golub, Gene H.; Wilkinson, J. H. (1976). "Ill-conditioned eigensystems and the computation of the Jordan normal form". SIAM Review. 18 (4): 578­619. Holt, Derek; Rumynin, Dmitriy (2009), Algebra I ­ Advanced Linear Algebra (MA251) Lecture Notes (http://homepages.warwick.ac.uk/ ~masdf/alg1/lec_notes_revised_at_the_end.pdf) (PDF) Horn, Roger A.; Johnson, Charles R. (1985), Matrix Analysis, Cambridge University Press, ISBN 978-0-521-38632-6 James, Glenn; James, Robert C. (1976), Mathematics Dictionary (2nd ed.), Van Nostrand Reinhold MacLane, Saunders; Birkhoff, Garrett (1967), Algebra, Macmillan Publishers Michel, Anthony N.; Herget, Charles J. (1993), Applied Algebra and Functional Analysis, Dover Publications Nering, Evar D. (1970), Linear Algebra and Matrix Theory (2nd ed.), New York: Wiley, LCCN 76091646 (https://lccn.loc.gov/76091646 ) Shafarevich, I. R.; Remizov, A. O. (2012), Linear Algebra and Geometry, Springer, ISBN 978-3-642-30993-9 Shilov, Georgi E. (1977), Linear Algebra, Dover Publications Jordan Canonical Form article at mathworld.wolfram.com (http://mathworld.wolfram.com/JordanCanonicalForm.html)

Schur decomposition
In the mathematical discipline of linear algebra, the Schur decomposition or Schur triangulation, named after Issai Schur, is a matrix decomposition.
Contents
Statement Proof Notes Computation Applications Generalized Schur decomposition References
Statement
The Schur decomposition reads as follows: if A is a n × n square matrix with complex entries, then A can be expressed as[1][2][3]
where Q is a unitary matrix (so that its inverse Q-1 is also the conjugate transpose Q* of Q), and U is an upper triangular matrix, which is called a Schur form of A. Since U is similar to A, it has the same spectrum, and since it is triangular, those eigenvalues are the diagonal entries of U. The Schur decomposition implies that there exists a nested sequence of A-invariant subspaces {0} = V0  V1  ...  Vn = Cn, and that there exists an ordered orthonormal basis (for the standard Hermitian form of Cn) such that the first i basis vectors span Vi for each i occurring in the nested sequence. Phrased somewhat differently, the first part says that a linear operator J on a complex finitedimensional vector space stabilizes a complete flag (V1,...,Vn).
Proof
A constructive proof for the Schur decomposition is as follows: every operator A on a complex finite-dimensional vector space has an eigenvalue , corresponding to some eigenspace V. Let V be its orthogonal complement. It is clear that, with respect to this orthogonal decomposition, A has matrix representation (one can pick here any orthonormal bases Z1 and Z2 spanning V and V respectiv ely )
where I is the identity operator on V. The above matrix would be upper-triangular except for the A22 block. But exactly the same procedure can be applied to the sub-matrix A22, viewed as an operator on V, and its submatrices. Continue this way n times. Thus the space Cn will be exhausted and the procedure has yielded the desired result.
The above argument can be slightly restated as follows: let  be an eigenvalue of A, corresponding to some eigenspace V. A induces an operator T on the quotient space Cn modulo V. This operator is precisely the A22 submatrix from above. As before, T would have an eigenspace, say W  Cn modulo V. Notice the preimage of W under the quotient map is an invariant subspace of A that contains V. Continue this way until the resulting quotient space has dimension 0. Then the successive preimages of the eigenspaces found at each step form a flag that A stabilizes.
Notes

Although every square matrix has a Schur decomposition, in general this decomposition is not unique. For example, the eigenspace V can have dimension > 1, in which case any orthonormal basis for V would lead to the desired result.
Write the triangular matrix U as U = D + N, where D is diagonal and N is strictly upper triangular (and thus a nilpotent matrix). The diagonal matrix D contains the eigenvalues of A in arbitrary order (hence its Frobenius norm, squared, is the sum of the squared moduli of the eigenvalues of A, while the Frobenius norm of A, squared, is the sum of the squared singular values of A). The nilpotent part N is generally not unique either, but its Frobenius norm is uniquely determined by A (just because the Frobenius norm of A is equal to the Frobenius norm of U = D + N).
It is clear that if A is a normal matrix, then U from its Schur decomposition must be a diagonal matrix and the column vectors of Q are the eigenvectors of A. Therefore, the Schur decomposition extends the spectral decomposition. In particular, if A is positive definite, the Schur decomposition of A, its spectral decomposition, and its singular value decomposition coincide.
A commuting family {Ai} of matrices can be simultaneously triangularized, i.e. there exists a unitary matrix Q such that, for every Ai in the given family, Q Ai Q* is upper triangular. This can be readily deduced from the above proof. Take element A from {Ai} and again consider an eigenspace VA. Then VA is invariant under all matrices in {Ai}. Therefore, all matrices in {Ai} must share one common eigenvector in VA. Induction then proves the claim. As a corollary, we have that every commuting family of normal matrices can be simultaneously diagonalized.
In the infinite dimensional setting, not every bounded operator on a Banach space has an invariant subspace. However, the uppertriangularization of an arbitrary square matrix does generalize to compact operators. Every compact operator on a complex Banach space has a nest of closed invariant subspaces.

Computation
The Schur decomposition of a given matrix is numerically computed by the QR algorithm or its variants. In other words, the roots of the characteristic polynomial corresponding to the matrix are not necessarily computed ahead in order to obtain its Schur decomposition. Conversely, the QR algorithm can be used to compute the roots of any given characteristic polynomial by finding the Schur decomposition of its companion matrix. Similarly, the QR algorithm is used to compute the eigenvalues of any given matrix, which are the diagonal entries of the upper triangular matrix of the Schur decomposition. See the Nonsymmetric Eigenproblems section in LAPACK Users' Guide.[4]

Applications
Lie theory applications include:
Every invertible operator is contained in a Borel group. Every operator fixes a point of the flag manifold.

Generalized Schur decomposition

Given square matrices A and B, the generalized Schur decomposition factorizes both matrices as

and , where Q and

Z are unitary, and S and T are upper triangular. The generalized Schur decomposition is also sometimes called the QZ decomposition.[2]:375

The generalized eigenvalues that solve the generalized eigenvalue problem

(where x is an unknown nonzero vector) can be

calculated as the ratio of the diagonal elements of S to those of T. That is, using subscripts to denote matrix elements, the ith generalized

eigenvalue satisfies

.

References

Taylor series

In mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.
The concept of a Taylor series was formulated by the Scottish mathematician James Gregory and formally introduced by the English mathematician Brook Taylor in 1715. If the Taylor series is centered at zero, then that series is also called a Maclaurin series, named after the Scottish mathematician Colin Maclaurin, who made extensive use of this special case of Taylor series in the 18th century.
A function can be approximated by using a finite number of terms of its Taylor series. Taylor's theorem gives quantitative estimates on the error introduced by the use of such an approximation. The polynomial formed by taking some initial terms of the Taylor series is called a Taylor polynomial. The Taylor series of a function is the limit of that function's Taylor polynomials as the degree increases, provided that the limit exists. A function may not be equal to its Taylor series, even if its Taylor series converges at every point. A function that is equal to its Taylor series in an open interval (or a disc in the complex plane) is known as an analytic function in that interval.

As the degree of the Taylor polynomial rises, it approaches the correct function. This image shows
sin x and its Taylor approximations, polynomials of
degree 1, 3, 5, 7, 9, 11 and 13.

Contents
Definition Examples History Analytic functions Approximation error and convergence
Generalization
List of Maclaurin series of some common functions Exponential function Natural logarithm Geometric series Binomial series Trigonometric functions Hyperbolic functions
Calculation of Taylor series First example Second example Third example
Taylor series as definitions Taylor series in several variables
Example
Comparison with Fourier series See also Notes References External links

Definition
The Taylor series of a real or complex-valued function f(x) that is infinitely differentiable at a real or complex number a is the power
series

which can be written in the more compact sigma notation as

where n! denotes the factorial of n and f(n)(a) denotes the nth derivative of f evaluated at the point a. The derivative of order zero of f is defined to be f itself and (x - a)0 and 0! are both defined to be 1. When a = 0, the series is also called a Maclaurin series.[1]

Examples

The Taylor series for any polynomial is the polynomial itself.

T he

Maclaurin

series

for

1

1 -

x

is

the

geometric

series

so

the

Tay

lor

series

for

1 x

at

a

=

1

is

By integrating the above Maclaurin series, we find the Maclaurin series for log(1 - x), where log denotes the natural logarithm:

and the corresponding Taylor series for log x at a = 1 is and more generally, the corresponding Taylor series for log x at some a = x0 is:

The Taylor series for the exponential function ex at a = 0 is

The above expansion holds because the derivative of ex with respect to x is also ex and e0 equals 1. This leaves the terms (x - 0)n in the numerator and n! in the denominator for each term in the infinite sum.
History
The Greek philosopher Zeno considered the problem of summing an infinite series to achieve a finite result, but rejected it as an impossibility: the result was Zeno's paradox. Later, Aristotle proposed a philosophical resolution of the paradox, but the mathematical content was apparently unresolved until taken up by Archimedes, as it had been prior to Aristotle by the Presocratic Atomist Democritus. It was through Archimedes's method of exhaustion that an infinite number of progressive subdivisions could be performed to achieve a finite result.[2] Liu Hui independently employed a similar method a few centuries later.[3]
In the 14th century, the earliest examples of the use of Taylor series and closely related methods were given by Madhava of Sangamagrama.[4][5] Though no record of his work survives, writings of later Indian mathematicians suggest that he found a number of special cases of the Taylor series, including those for the trigonometric functions of sine, cosine, tangent, and arctangent. The Kerala School of Astronomy and Mathematics further expanded his works with various series expansions and rational approximations until the 16th century.

In the 17th century, James Gregory also worked in this area and published several Maclaurin series. It was not until 1715 however that a general method for constructing these series for all functions for which they exist was finally provided by Brook Taylor,[6] after whom the series are now named.
The Maclaurin series was named after Colin Maclaurin, a professor in Edinburgh, who published the special case of the Taylor result in the 18th century.
Analytic functions
If f(x) is given by a convergent power series in an open disc (or interval in the real line) centered at b in the complex plane, it is said to be analytic in this disc. Thus for x in this disc, f is given by a convergent power series

Differentiating by x the above formula n times, then setting x = b gives:

and so the power series expansion agrees with the Taylor series. Thus a
function is analytic in an open disc centered at b if and only if its Taylor series
converges to the value of the function at each point of the disc.

I f f(x) is equal to its Taylor series for all x in the complex plane, it is called entire. The polynomials, exponential function ex, and the trigonometric
functions sine and cosine, are examples of entire functions. Examples of

The function e(-1/x2) is not analytic at x = 0: the

functions that are not entire include the square root, the logarithm, the trigonometric function tangent, and its inverse, arctan. For these functions the

Taylor series is identically 0, although the function is not.

Taylor series do not converge if x is far from b. That is, the Taylor series

diverges at x if the distance between x and b is larger than the radius of

convergence. The Taylor series can be used to calculate the value of an entire function at every point, if the value of the function, and of

all of its derivatives, are known at a single point.

Uses of the Taylor series for analytic functions include:

1. The partial sums (the Taylor polynomials) of the series can be used as approximations of the function. These approximations are good if sufficiently many terms are included.
2. Differentiation and integration of power series can be performed term by term and is hence particularly easy.
3. An analytic function is uniquely extended to a holomorphic function on an open disk in the complex plane. This makes the machinery of complex analysis available.
4. The (truncated) series can be used to compute function values numerically, (often by recasting the polynomial into the Chebyshev form and evaluating it with the Clenshaw algorithm).
5. Algebraic operations can be done readily on the power series representation; for instance, Euler's formula follows from Taylor series expansions for trigonometric and exponential functions. This result is of fundamental importance in such fields as harmonic analysis.
6. Approximations using the first few terms of a Taylor series can make otherwise unsolvable problems possible for a restricted domain; this approach is often used in physics.

Approximation error and convergence
Pictured on the right is an accurate approximation of sin x around the point x = 0. The pink curve is a polynomial of degree seven:

The error in this approximation is no more than |x9!|9. In particular, for -1 < x < 1, the error is less than 0.000003.
In contrast, also shown is a picture of the natural logarithm function log(1 + x) and some of its Taylor polynomials around a = 0. These approximations converge to the function only in the region -1 < x  1; outside of this region the higher-degree Taylor polynomials are
worse approximations for the function. This is similar to Runge's phenomenon.

The error incurred in approximating a function by its nth-degree Taylor polynomial is called the remainder or residual and is denoted by the function Rn(x). Taylor's theorem can be used to obtain a bound on the size of the remainder.
In general, Taylor series need not be convergent at all. And in fact the set of functions with a convergent Taylor series is a meager set in
the Fréchet space of smooth functions. And even if the Taylor series of a function f does converge, its limit need not in general be equal to the value of the function f(x). For example, the function

is infinitely differentiable at x = 0, and has all derivatives zero there. Consequently, the Taylor series of f(x) about x = 0 is identically zero. However, f(x) is not the zero function, so does not equal its Taylor series around the origin. Thus, f(x) is an example of a non-
analytic smooth function.

I n real analysis, this example shows that there are infinitely differentiable
functions f(x) whose Taylor series are not equal to f(x) even if they converge.
By contrast, the holomorphic functions studied in complex analysis always possess a convergent Taylor series, and even the Taylor series of meromorphic functions, which might have singularities, never converge to a value different
from the function itself. The complex function e-1/z2, however, does not approach 0 when z approaches 0 along the imaginary axis, so it is not
continuous in the complex plane and its Taylor series is undefined at 0.

More generally, every sequence of real or complex numbers can appear as coefficients in the Taylor series of an infinitely differentiable function defined on the real line, a consequence of Borel's lemma. As a result, the radius of convergence of a Taylor series can be zero. There are even infinitely differentiable functions defined on the real line whose Taylor series have a radius of convergence 0 everywhere.[7]

The sine function (blue) is closely approximated by its Taylor polynomial of degree 7 (pink) for a full period centered at the origin.

A function cannot be written as a Taylor series centered at a singularity; in
these cases, one can often still achieve a series expansion if one allows also
negative powers of the variable x; see Laurent series. For example, f(x) = e-1/x2 can be written as a Laurent series.

Generalization
There is, however, a generalization[8][9] of the Taylor series that does converge
to the value of the function itself for any bounded continuous function on (0,),
using the calculus of finite differences. Specifically, one has the following
theorem, due to Einar Hille, that for any t > 0,

Here hn is the nth finite difference operator with step size h. The series is
precisely the Taylor series, except that divided differences appear in place of
differentiation: the series is formally similar to the Newton series. When the
function f is analytic at a, the terms in the series converge to the terms of the
Taylor series, and in this sense generalizes the usual Taylor series.

The Taylor polynomials for log(1 + x) only provide accurate approximations in the range -1 < x  1. For x > 1, Taylor polynomials of higher degree
provide worse approximations.

In general, for any infinite sequence ai, the following power series identity holds:

So in particular,

The series on the right is the expectation value of f(a + X), where X is a Poisson-distributed random variable that takes the value jh with probability e-t/h·(t/jh!)j. Hence,

The law of large numbers implies that the identity holds.[10]
List of Maclaurin series of some common functions
Several important Maclaurin series expansions follow.[11] All these expansions are valid for complex arguments x.

Exponential function
The exponential function (with base e) has Maclaurin series .
It converges for all x.

Natural logarithm
The natural logarithm (with base e) has Maclaurin series

The Taylor approximations for log(1 + x) (black). For x > 1, the approximations diverge.

They converge for .
Geometric series
The geometric series and its derivatives have Maclaurin series

All are convergent for . These are special cases of the binomial series given in the next section.
Binomial series
The binomial series is the power series

The exponential function ex (in blue), and the sum of the first n + 1 terms of
its Taylor series at 0 (in red).

whose coefficients are the generalized binomial coefficients

(If n = 0, this product is an empty product and has value 1.) It converges for for any real or complex number .

When



=

-1,

this

is

essentially

the

infinite

geometric

series

mentioned

in

the

previous

section.

T he

special

cases



=

1 2

and



=

-

1 2

give

the square root function and its inverse:

Trigonometric functions
The usual trigonometric functions and their inverses have the following Maclaurin series:
All angles are expressed in radians. The numbers Bk appearing in the expansions of tan x are the Bernoulli numbers. The Ek in the expansion of sec x are Euler numbers.
Hyperbolic functions
The hyperbolic functions have Maclaurin series closely related to the series for the corresponding trigonometric functions:
The numbers Bk appearing in the series for tanh x are the Bernoulli numbers.
Calculation of Taylor series
Several methods exist for the calculation of Taylor series of a large number of functions. One can attempt to use the definition of the Taylor series, though this often requires generalizing the form of the coefficients according to a readily apparent pattern. Alternatively, one can use manipulations such as substitution, multiplication or division, addition or subtraction of standard Taylor series to construct the Taylor series of a function, by virtue of Taylor series being power series. In some cases, one can also derive the Taylor series by repeatedly applying integration by parts. Particularly convenient is the use of computer algebra systems to calculate Taylor series.
First example
In order to compute the 7th degree Maclaurin polynomial for the function
,
one may first rewrite the function as
.
The Taylor series for the natural logarithm is (using the big O notation)

and for the cosine function

.

The latter series expansion has a zero constant term, which enables us to substitute the second series into the first one and to easily
omit terms of higher order than the 7th degree by using the big O notation:

Since the cosine is an even function, the coefficients for all the odd powers x, x3, x5, x7, ... have to be zero.
Second example
Suppose we want the Taylor series at 0 of the function We have for the exponential function
and, as in the first example,
Assume the power series is Then multiplication with the denominator and substitution of the series of the cosine yields
Collecting the terms up to fourth order yields The values of can be found by comparison of coefficients with the top expression for , yielding:
Third example
Here we employ a method called "indirect expansion" to expand the given function. This method uses the known Taylor expansion of
the exponential function. In order to expand (1 + x)ex as a Taylor series in x, we use the known Taylor series of function ex:
T hus,

Taylor series as definitions
Classically, algebraic functions are defined by an algebraic equation, and transcendental functions (including those discussed above) are defined by some property that holds for them, such as a differential equation. For example, the exponential function is the function which is equal to its own derivative everywhere, and assumes the value 1 at the origin. However, one may equally well define an analytic function by its Taylor series. Taylor series are used to define functions and "operators" in diverse areas of mathematics. In particular, this is true in areas where the classical definitions of functions break down. For example, using Taylor series, one may extend analytic functions to sets of matrices and operators, such as the matrix exponential or matrix logarithm. In other areas, such as formal analysis, it is more convenient to work directly with the power series themselves. Thus one may define a solution of a differential equation as a power series which, one hopes to prove, is the Taylor series of the desired solution.
Taylor series in several variables
The Taylor series may also be generalized to functions of more than one variable with[12][13]
For example, for a function that depends on two variables, x and y, the Taylor series to second order about the point (a, b) is
where the subscripts denote the respective partial derivatives. A second-order Taylor series expansion of a scalar-valued function of more than one variable can be written compactly as
where D f(a) is the gradient of f evaluated at x = a and D2 f(a) is the Hessian matrix. Applying the multi-index notation the Taylor
series for several variables becomes
which is to be understood as a still more abbreviated multi-index version of the first equation of this paragraph, again in full analogy to the single variable case.
Example
In order to compute a second-order Taylor series expansion around point (a, b) = (0, 0) of the function
one first computes all the necessary partial derivatives:

Evaluating these derivatives at the origin gives the Taylor coefficients
Substituting these values in to the general formula produces

Second-order Taylor series
approximation (in orange) of a
function f(x,y) = ex log(1 + y)
around the origin.

Since log(1 + y) is analytic in |y| < 1, we have
Comparison with Fourier series
The trigonometric Fourier series enables one to express a periodic function (or a function defined on a closed interval [a,b]) as an
infinite sum of trigonometric functions (sines and cosines). In this sense, the Fourier series is analogous to Taylor series, since the latter allows one to express a function as an infinite sum of powers. Nevertheless, the two series differ from each other in several relevant issues:
Obviously the finite truncations of the Taylor series of f(x) about the point x = a are all exactly equal to f at a. In contrast, the Fourier
series is computed by integrating over an entire interval, so there is generally no such point where all the finite truncations of the series are exact. Indeed, the computation of Taylor series requires the knowledge of the function on an arbitrary small neighbourhood of a point, whereas the computation of the Fourier series requires knowing the function on its whole domain interval. In a certain sense one could say that the Taylor series is "local" and the Fourier series is "global". The Taylor series is defined for a function which has infinitely many derivatives at a single point, whereas the Fourier series is defined
for any integrable function. In particular, the function could be nowhere differentiable. (For example, f(x) could be a Weierstrass
function.) The convergence of both series has very different properties. Even if the Taylor series has positive convergence radius, the resulting series may not coincide with the function; but if the function is analytic then the series converges pointwise to the function, and uniformly on every compact subset of the convergence interval. Concerning the Fourier series, if the function is square-integrable then the series converges in quadratic mean, but additional requirements are needed to ensure the pointwise or uniform convergence (for instance, if the function is periodic and of class C1 then the convergence is uniform). Finally, in practice one wants to approximate the function with a finite number of terms, say with a Taylor polynomial or a partial sum of the trigonometric series, respectively. In the case of the Taylor series the error is very small in a neighbourhood of the point where it is computed, while it may be very large at a distant point. In the case of the Fourier series the error is distributed along the domain of the function.
See also
Asymptotic expansion Generating function Laurent series Madhava series Newton's divided difference interpolation Padé approximant Puiseux series

Notes
References
Abramowitz, Milton; Stegun, Irene A. (1970), Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, New York: Dover Publications, Ninth printing Thomas, George B., Jr.; Finney, Ross L. (1996), Calculus and Analytic Geometry (9th ed.), Addison Wesley, ISBN 0-201-53174-7 Greenberg, Michael (1998), Advanced Engineering Mathematics (2nd ed.), Prentice Hall, ISBN 0-13-321431-1
External links
Hazewinkel, Michiel, ed. (2001) [1994], "Taylor series" (https://www.encyclopediaofmath.org/index.php?title=p/t092320), Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 Weisstein, Eric W. "Taylor Series" (http://mathworld.wolfram.com/TaylorSeries.html). MathWorld. Taylor polynomial (http://blog.ivank.net/taylor-polynomial-clarified.html) - practical introduction Madhava of Sangamagramma (http://www-groups.dcs.st-and.ac.uk/~history/Projects/Pearce/Chapters/Ch9_3.html) "Discussion of the Parker-Sochacki Method (http://csma31.csm.jmu.edu/physics/rudmin/ParkerSochacki.htm)" Another Taylor visualisation (https://web.archive.org/web/20070605020930/http://stud3.tuwien.ac.at/~e0004876/taylor/Taylor_en.html) -- where you can choose the point of the approximation and the number of derivatives Taylor series revisited for numerical methods (http://numericalmethods.eng.usf.edu/topics/taylor_series.html) at Numerical Methods for the STEM Undergraduate (http://numericalmethods.eng.usf.edu) Cinderella 2: Taylor expansion (http://cinderella.de/files/HTMLDemos/2C02_Taylor.html) Taylor series (http://www.sosmath.com/calculus/tayser/tayser01/tayser01.html) Inverse trigonometric functions Taylor series (http://www.efunda.com/math/taylor_series/inverse_trig.cfm) "Essence of Calculus: Taylor series" (https://www.youtube.com/watch?v=3d6DsjIBzJ4&index=11&list=PLZHQObOWTQDMsr9K-rj53D wVRMYO3t5Yr) ­ via YouTube.

Taylor Series Approximation
A Taylor series approximation uses a Taylor series to represent a number as a polynomial that has a very similar value to the number in a neighborhood around a specified \(x\) value: \[f(x) = f(a)+\frac {f'(a)}{1!} (x-a)+ \frac{f''(a)}{2!} (x-a)^2+\frac{f^{(3)}(a)}{3!}(x-a)^3+ \cdots.\] Taylor series are extremely powerful tools for approximating functions that can be difficult to compute otherwise, as well as evaluating infinite sums and integrals by recognizing Taylor series.
If only concerned about the neighborhood very close to the origin, the \(n=2\) approximation represents the sine wave sufficiently, and no higher orders are direly needed. [1]
Suggested steps for approximating values: 1. Identify a function to resemble the operation on the number in question. 2. Choose \(a\) to be a number that makes \(f(a)\) easy to compute. 3. Select \(x\) to make \(f(x)\) the number being approximated.
EXAMPLE
Using the first three terms of the Taylor series expansion of \(f(x) = \sqrt[3]{x}\) centered at \(x = 8\), approximate \(\sqrt[3]{8.1}:\) \[f(x) = \sqrt[3]{x} \approx 2 + \frac{(x - 8)}{12} - \frac{(x - 8)^2}{288} .\] The first three terms shown will be sufficient to provide a good approximation for \(\sqrt[3]{x}\). Evaluating this sum at \(x = 8.1\) gives an approximation for \(\sqrt[3]{8.1}:\) \[\begin{align} f(8.1) = \sqrt[3]{8.1} &\approx 2 + \frac{(8.1 - 8)}{12} - \frac{(8.1 - 8)^2}{288} \\ &=\color{blue}{2.008298}\color{red} {61111}\ldots \\ \\ \sqrt[3]{8.1} &={ \color{blue}{2.008298}\color{red}{85025}\dots}. \end{align}\] With just three terms, the formula above was able to approximate \(\sqrt[3]{8.1}\) to six decimal places of accuracy. \(_\square\)
EXAMPLE
Using the quadratic Taylor polynomial for \(f(x) = \frac{1}{x^2},\) approximate the value of \(\frac{1}{4.41}.\) The quadratic Taylor polynomial is \[P_2(x) = f(a)+\frac {f'(a)}{1!} (x-a)+ \frac{f''(a)}{2!} (x-a)^2.\] First, write down the derivatives needed for the Taylor expansion:

\[f(x) = \frac{1}{x^2},\quad f'(x) = \frac{-2}{x^3},\quad f''(x) = \frac{6}{x^4}.\] But what about \(a\) and \(x?\) Choose \(a\) so that the values of the derivatives are easy to calculate. Rewriting the approximated value as \[4.41 = (2+0.1)^2\] implies \(a = 2\) and \(x = 2.1.\) \[\begin{align} P_2(2.1) &= f(2)+\frac {f'(2)}{1!} (2.1-2)+ \frac{f''(2)}{2!} (2.1-2)^2\\ &= \frac14 +\frac {\frac{-2}{8}}{1!} (2.1-2)+ \frac{\frac{6}{16}}{2!} (2.1-2)^2 \\ &= \frac14 + \frac {-1}{4}(0.1) + \frac{3}{16}(0.01)\\ &= 0.25 - 0.025 + 0.001875 \\ &= 0.226875. \end{align}\] The actual value is \[\frac{1}{4.41} = 0.226757...,\] so the approximation is only off by about 0.05%. \(_\square\)
References
1. IkamusumeFan, . Sine GIF. Retrieved June 1, 2016, from https://commons.wikimedia.org/wiki/File:Sine_GIF.gif
Cite as: Taylor Series Approximation. Brilliant.org. Retrieved from https://brilliant.org/wiki/taylor-series-approximation/

Transcendental function
A transcendental function is an analytic function that does not satisfy a polynomial equation, in contrast to an algebraic function.[1][2] In other words, a transcendental function "transcends" algebra in that it cannot be expressed in terms of a finite sequence of the algebraic operations of addition, multiplication, and root extraction.
Examples of transcendental functions include the exponential function, the logarithm, and the trigonometric functions.

Contents
Definition History Examples Algebraic and transcendental functions Transcendentally transcendental functions Exceptional set Dimensional analysis See also References External links

Definition
Formally, an analytic function (z) of one real or complex variable z is transcendental if it is algebraically independent of that variable.[3] This can be extended to functions of several variables.

History
The transcendental functions sine and cosine were tabulated from physical measurements in antiquity, as evidenced in Greece (Hipparchus) and India (jya and koti-jya). In describing the trigonometric tables used by Ptolemy, Olaf Pedersen wrote:

The mathematical notion of continuity as an explicit concept is unknown to Ptolemy. That he, in fact, treats these functions as continuous appears from his unspoken presumption that it is possible to determine a value of the dependent variable corresponding to any value of the independent variable by the simple process of linear interpolation.[4]

A revolutionary understanding of these circular functions occurred in the 17th century and was explicated by Leonhard Euler in 1748 in his Introduction to the Analysis of the Infinite. These ancient transcendental functions became known as continuous functions through quadrature of the rectangular hyperbola xy = 1 by Grégoire de Saint-Vincent in 1647, two millennia after Archimedes had produced The Quadrature of the Parabola.

The area under the hyperbola was shown to have the scaling property of constant area for a constant ratio of bounds. The natural logarithm function so described was of limited service until 1748 when Leonhard Euler related it to functions where a constant is raised to a variable exponent, such as the exponential function where the constant base is e. By introducing these transcendental functions and noting the bijection property that implies an inverse function, some facility was provided for algebraic manipulations of the natural logarithm even if it is not an algebraic function.

The exponential function is written

Euler identified it with the infinite series

where k! denotes the factorial of k.

The even and odd terms of this series provide sums denoting cosh x and sinh x, so that

These transcendental hyperbolic

functions can be converted into circular functions sine and cosine by introducing (-1)k into the series, resulting in alternating series.

After Euler, mathematicians view the sine and cosine this way to relate the transcendence to logarithm and exponent functions, often

through Euler's formula in complex number arithmetic.

Examples
The following functions are transcendental:

In particular, for 2 if we set c equal to e, the base of the natural logarithm, then we get that ex is a transcendental function. Similarly, if

we set c equal to e in 5, then we get that

(that is, the natural logarithm) is a transcendental function.

Algebraic and transcendental functions
The most familiar transcendental functions are the logarithm, the exponential (with any non-trivial base), the trigonometric, and the hyperbolic functions, and the inverses of all of these. Less familiar are the special functions of analysis, such as the gamma, elliptic, and zeta functions, all of which are transcendental. The generalized hypergeometric and Bessel functions are transcendental in general, but algebraic for some special parameter values.
A function that is not transcendental is algebraic. Simple examples of algebraic functions are the rational functions and the square root function, but in general, algebraic functions cannot be defined as finite formulas of the elementary functions.[5]
T he indefinite integral of many algebraic functions is transcendental. For example, the logarithm function arose from the reciprocal function in an effort to find the area of a hyperbolic sector.
Differential algebra examines how integration frequently creates functions that are algebraically independent of some class, such as when one takes polynomials with trigonometric functions as variables.

Transcendentally transcendental functions
Most familiar transcendental functions, including the special functions of mathematical physics, are solutions of algebraic differential equations. Those that are not, such as the gamma and the zeta functions, are called transcendentally transcendental or hypertranscendental functions.

Exceptional set
If is an algebraic function and is an algebraic number then is also an algebraic number. The converse is not true: there are entire transcendental functions such that is an algebraic number for any algebraic [6] For a given transcendental function the set of algebraic numbers giving algebraic results is called the exceptional set of that function.[7][8] Formally it is defined by:

In many instances the exceptional set is fairly small. For example,

this was proved by Lindemann in 1882. In particular exp(1)

= e is transcendental. Also, since exp(i) = -1 is algebraic we know that i cannot be algebraic. Since i is algebraic this implies that  is a

transcendental number.

In general, finding the exceptional set of a function is a difficult problem, but if it can be calculated then it can often lead to results in transcendental number theory. Here are some other known exceptional sets:

Klein's j-invariant

where H is the upper half-plane, and [Q(): Q] is the degree of the number field Q(). This result is due to Theodor Schneider.[9]
Exponential function in base 2:

,
This result is a corollary of the Gelfond­Schneider theorem, which states that if is algebraic, and is algebraic and irrational then is transcendental. Thus the function 2x could be replaced by cx for any algebraic c not equal to 0 or 1. Indeed, we have:
A consequence of Schanuel's conjecture in transcendental number theory would be that
A function with empty exceptional set that does not require assuming Schanuel's conjecture is While calculating the exceptional set for a given function is not easy, it is known that given any subset of the algebraic numbers, say A, there is a transcendental function whose exceptional set is A.[10] The subset does not need to be proper, meaning that A can be the set of algebraic numbers. This directly implies that there exist transcendental functions that produce transcendental numbers only when given transcendental numbers. Alex Wilkie also proved that there exist transcendental functions for which first-order-logic proofs about their transcendence do not exist by providing an exemplary analytic function.[11]
Dimensional analysis
I n dimensional analysis, transcendental functions are notable because they make sense only when their argument is dimensionless (possibly after algebraic reduction). Because of this, transcendental functions can be an easy-to-spot source of dimensional errors. For example, log(5 meters) is a nonsensical expression, unlike log(5 meters / 3 meters) or log(3) meters. One could attempt to apply a logarithmic identity to get log(5) + log(meters), which highlights the problem: applying a non-algebraic operation to a dimension creates meaningless results.
See also
Complex function Function (mathematics) Generalized function List of special functions and eponyms List of types of functions Rational function Special functions
References
External links
Definition of "Transcendental function" in the Encyclopedia of Math (https://www.encyclopediaofmath.org/index.php/Transcendental_fun ction)

