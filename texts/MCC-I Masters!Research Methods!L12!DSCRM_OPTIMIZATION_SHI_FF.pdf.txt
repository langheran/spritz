Optimization based Data Mining
material from Prof. Yong Shi
further details in his book: Optimization Based Data Mining:
Theory and Applications

Classification function

Objective
Function

Constraints

Nondominated
Potential Solution
Convex Structure

2018/4/13

Optimization

Classification Criteria
Classifier
Preferences

BSC@YShi-Jan-2014

Data Mining

Classification
· Use a training data set for predetermined
classes
· Develop a separation model with rules on the
training set
· Apply the model to classify unknown objects
· Discover knowledge

training set
2018/4/13

modelBSC@YShi-Jan-2014
unknown objects knowledge

Linear System Approaches
a1

G1:

G2:

2018/4/13

1
2
:
k
k+1
k+2
:
n

A11
A21

a2 Linear Transformed
Scores
Ai X = b
A12
A22

A1 X*
A2 X*
:

Ak1
Ak+1,1
Ak+2,1

Ak2
Ak+2,2
Ak+2,2

Ak X*
Ak+1 X*
Ak+2 X*
:

An1

An2

BSC@YShi-Jan-2014

AnX*

Linear System Approaches
· Example: Consider a credit-rating problem with
two variables and two cases:
· a1 = salary and a2 = age. Let the boundary b = 10
Cases (Obs) a1

a2 Boundary found best
LP Score
coef.
b = 10
x1
x2

A1

6

8

10

.4

.6 7.2

A2

8

12

10

.4

.6 10.4

2018/4/13

BSC@YShi-Jan-2014

Linear System Approaches
· Find the best (x1* , x2*) to compare
· Ai X* = a1 x1* + a2 x2* with b = 10
· We see A1 X* = 7.2 is Bad (< 10) and
·
A2 X* = 10.4 is Good ( > 10)

i

i

Bad
7.2
2018/4/13

Good

10.4
Ai X* = 10
Perfect Separation ( = 0)
BSC@YShi-Jan-2014

Linear System Approaches
Example: Overlapping
Ai X* = b







i
i

i

i

Bad

2018/4/13

Good

Ai X* = b-

Ai X* = b+

BSC@YShi-Jan-2014

Linear System Approaches:
Linear Programming
· Linear programming has been used for
Classification in Data Mining.
· Given two attributes {a1 , a2} and
· two groups {G1 , G2}, with the observation
· Ai = (Ai1, Ai2),
­ we want to find a scalar b and nonzero
vector X = (x1, x2) such that
Ai X  b, Ai  G1 and Ai X  b, Ai  G2 have
the fewest number of violated constraints.
2018/4/13

BSC@YShi-Jan-2014

Linear System Approaches
· Let
· i = the overlapping of two-group (classes) boundary
for case Ai (external measurement);
·  = the max overlapping of two-group (classes)
boundary for all cases Ai (i <  );
· i = the distance of case Ai from its adjusted
boundary (internal measurement);
·  = the min distance of all cases Ai to the adjusted
boundary ( i >  );
· hi = the penalties for i ("cost" of misclassification);
· ki = the penalties for i ("cost" of misclassification);
2018/4/13

BSC@YShi-Jan-2014

Linear System Approaches:
Multi-Criteria Linear Programming

·
·
·
·
·

Multi-Criteria Linear programming considers
to simultaneously minimize the total
overlapping degree and maximize the total
distance from the boundary of two groups
(Shi, Wise, Luo, and Lin 2001):
Minimize ii and Maximize ii
Subject to
Ai X = b + i - i, Ai  B,
Ai X = b - i + i, Ai  G,
where Ai are given,
X
and
b are
unrestricted, and i and i  0.

2018/4/13

BSC@YShi-Jan-2014

Linear System Approaches:
Multi-Criteria Linear Programming
Ai X = b1

Ai X = b2

i1
i1

i2

 i1

G1

i2

 i1

b1 - 1

 i2

G2
b1 + 1

b2 - 2

Three-group MC Model
2018/4/13

BSC@YShi-Jan-2014

i2

b2 + 2

G3

Linear System Approaches:
Multi-Criteria Linear Programming
· To utilize the capability of computational power of
some commercial software on LP and non-LP
problems, we can find the compromise solution for
the separation problems (Yu 1973, Yu 1985, and Shi
and Yu 1989):
· Let
· * = the ideal overlapping of -i i;
· * = the ideal distance of i i.

· Then, we define the regret function as:
·
·
·
·

-d+ = ii + *, if -ii > *; otherwise, it is 0.
d - = * + ii, if -ii < *;otherwise, it is 0.
d  + = i i - *, if i i > *; otherwise, it is 0.
d - = * - i i, if i i < *; otherwise, it is 0.

2018/4/13

BSC@YShi-Jan-2014

Multi-Criteria Linear Programming
i

Min

(d+ + d - ) p + (d  + +d - ) p
(*, *)

-ii

0
2018/4/13

BSC@YShi-Jan-2014

Linear System Approaches:
Multi-Criteria Linear Programming
· Thus, the Multi-Criteria separation problem
becomes:
·
·
·
·
·
·
·

Min (d+ + d - ) p + (d  + +d - ) p
Subject to
* + ii = d - - d + ,
* - ii = d - - d+ ,
Ai X = b + i - i, Ai  G,
Ai X = b - i + i, Ai  B,
where Ai, *, and * are given, X and b are
unrestricted,
· and i , i , d - , d + , d - , d+  0.
2018/4/13

BSC@YShi-Jan-2014

Linear System Approaches:
Multi-Criteria Linear Programming
Algorithm:
·

Step 1
Use ReadCHD to convert both Training and Verifying
data into data mat.

·

Step 2
Use GroupDef to divide the observations within Training
data sets into s groups: G1, G2, ...., and Gs.

·

Step 3
Use sGModel to perform the separation task on the
training data. Here, PROC LP is called to calculate the MCLP model for
the best solution of the s-group classifier given the values of control
parameters.

·

Step 4
Use Score to produce the graphical representations of
training results. Step 3-4 will not terminate until the best training result
is found.

·

Step 5

2018/4/13

Use Predict to mine the s groups from Verifying data set.
BSC@YShi-Jan-2014

Example: A two-class data set of customer status
Age

Income

Student

Credit
Rating

Class:
buys_
computer

Training
results

Cases
A1

31... 40

high

no

fair

yes

success

A2

>40

medium

no

fair

yes

success

A3

> 40

low

yes

fair

yes

success

A4

31... 40

low

yes

excellent

yes

success

A5

30

low

yes

fair

yes

success

A6

>40

medium

yes

fair

yes

success

A7

30

medium

yes

excellent

yes

success

A8

31... 40

medium

no

excellent

yes

failure

A9

31... 40

high

yes

fair

yes

success

A10

30

high

no

fair

no

success

A11

30

high

no

excellent

no

success

A12

>40

low

yes

excellent

no

failure

A13

30

medium

no

fair

no

success

A14
>40
2018/4/13

medium

no

success

no
excellent
BSC@YShi-Jan-2014

Multi-Criteria Non-linear Programming
(Kou, Peng, Yan, Shi and Chen 2005):
Let i, j = the distance from case Ai to bj

2018/4/13

BSC@YShi-Jan-2014

Multi-Criteria Non-linear Programming

2018/4/13

BSC@YShi-Jan-2014

Non-linear Multi-Criteria Programming

2018/4/13

BSC@YShi-Jan-2014

Non-linear Multi-Criteria Programming

2018/4/13

BSC@YShi-Jan-2014

Other Multi-Criteria Non-linear Programming
(Zhang, Zhang and Shi 2005):
· Min w||  || pp - w ||  || pp = f (x, , , b ) = f ()
· s. t.
Ai X - i + i - b = 0, Ai  G1
Ai X + i - i - b = 0, Ai  G2
i , i  0 , i=1,...,n
·  =(1,..., n)T,  =(1,..., n)T
· = (x, , , b)

2018/4/13

BSC@YShi-Jan-2014

(1)

Other Multi-Criteria Non-linear Programming
Let
· ={(x, , , b): Ai X - i + i - b = 0; Ai  G1
Ai X + i - i - b = 0, Ai  G2 , i , i  0 },
Then,

2018/4/13

 0,   
X  
 ,   

BSC@YShi-Jan-2014

Other Multi-Criteria Non-linear Programming

·
·
·
·
·

(1) can be written as:
Min f ()+ X () = g() - h() (2)
g()= 0.5||||2+ w ||  || pp + X ()
h()= 0.5||||2 + w ||  || pp
where g(), h() are strong quadratic
functions.

2018/4/13

BSC@YShi-Jan-2014

Other Multi-Criteria Non-linear Programming
Algorithm:
· Given 0  R2n+m+1, (m=|G1G2|), for every k,
we solve the following quadratic program for
findingk+1 :
· quadratic program (Qk),
· min {0.5||||2+ w ||  || pp ­(H(k),)}
· H() gradient of H().
· The termination criterion is
· whenever ||k+1-k||  
2018/4/13

BSC@YShi-Jan-2014

Other Multi-Criteria Non-linear Programming

Theorem:
· The series of {0, ... k } resulted from the
above algorithm converges to a local
minimum of Problem (1).

2018/4/13

BSC@YShi-Jan-2014

Other Multi-Criteria Non-linear Programming
· More generally, we can consider
· Min w||  || pp - w ||  || qq
(3)
· s. t.  
· Then, we observe that:
· When p=q=1, (3) is a linear program;
· When p=2, q=1, (3) is a quadratic program ( many
methods can be applied to solve it);
· When p=1, q=2, (3) is a quadratic program and the
above algorithm can be terminated at a local
minimum of (3) with finite steps.
2018/4/13

BSC@YShi-Jan-2014

Support Vector Machines
Maximize the Margin between Bounding
Planes (adapted Olvi. L. Mangasarian)
x

w x' = b+1

A+
A-

2018/4/13

BSC@YShi-Jan-2014

w x' = b-1

2/||x||

Standard Support Vector Machine
Algebra of 2-Class Linearly Separable Case
(adapted Olvi. L. Mangasarian)

 Given n points in r dimensional space
 Represented by an n-by-r matrix A
 Membership of each Ai in class +1 or ­1 specified by:
 An nxn diagonal matrix D with +1 & -1 entries
Separate by two bounding planes w x' = b + 1;
Ai X  b + 1, Dii = +1,
Ai X  b - 1, Dii = -1.
Then,
D (AX ­ eb)  e, where e is a vector of ones
2018/4/13

BSC@YShi-Jan-2014

Standard Support Vector Machine
Formulation (adapted Olvi. L. Mangasarian)
Solve the quadratic program for some  > 0:
Min (/2) ||||2 + (1/2) ||x, b||2
s.t.
D (AX ­ eb)  e -  , where e is a vector of ones
Margin is maximized by minimizing (1/2) ||x, b||2

2018/4/13

BSC@YShi-Jan-2014

Connections between MCLP and Support
Vector Machines (SVM)

Let the Membership of each Ai in class +1 or ­1
specified by: An n-by-n diagonal matrix D with +1 & -1 entries
in MCLP Problem.
Then, the two-class constraints become:
Ai X  b + i - i, Dii = +1 (Bad),
Ai X  b - i + i, Dii = -1 (Good);
which can be written as

D (AX ­ eb)   - , where e is a vector of ones
2018/4/13

BSC@YShi-Jan-2014

Connections between MCLP and Support
Vector Machines (SVM)
 An

MCLP formulation:

Min w||  || - w||  ||
p

s.t.
D (AX ­ eb)   - 

2018/4/13

BSC@YShi-Jan-2014

p

MCQP

2018/4/13

BSC@YShi-Jan-2014

MCQP

2018/4/13

BSC@YShi-Jan-2014

MCQP
Lagrange function corresponding to Model 3
n
Wb 2 T
1
2 W n 2
L( X , b, , )  || X ||2   i  W  i  b   (Y ( <A  X> - eb )-e ' + )
i 1
2
2 i1
2

According to Wolfe Dual Theorem

 X L( X , b, , )  X  A Y  0
T

b L( X , b, , )  Wbb  eT Y  0
 L( X , b, , )  W  W e    0
2018/4/13

BSC@YShi-Jan-2014

MCQP
Introduce previous 3 equations to the constraints of Model 3

1 T
1
Y (( A  A )Y  e(e Y ))  (  W e)   'e
Wb
W
T

W
(  )e
W
 
I
1 T
T
 Y (( A  A )  ee )Y
W
Wb
'

Thus we will have:

X  A Y
*

2018/4/13

T

*

1 T *
b  e Y
Wb
*

 0,  A  G
i 1
*
*
(( X  A )  b ){
i
 0,  A  G
i 2

BSC@YShi-Jan-2014

MCQP with Kernel
Replace X by AT Y and AAT with K(A,AT) in Model 3
Model 3Minimize
n
W n 2
Wb 2
1
2
 i  W  i 
||  ||2 
b
i

1
i

1
2
2
2

,

Subject to:

Y (K(A,AT)Y - eb) =  'e - 

2018/4/13

BSC@YShi-Jan-2014

MCQP with Kernel
Lagrange function corresponding to Model 3
n
Wb 2 T
1 2 W n 2
L( , b, ,  )  ||  ||2   i  W  i  b   (Y ( K (A,A T )Y - eb )-e ' + )
i 1
2
2 i1
2
,

According to Wolfe Dual Theorem

 L( , b, ,  )    Y (K (A,A T )T Y  0
b L( , b, ,  )  Wbb  eT Y   0
 L( , b, ,  )  W  W e    0
2018/4/13

BSC@YShi-Jan-2014

MCQP with Kernel
Introduce previous 3 equations to the constraints of Model 3

1 T
1
'
Y ( K ( A, A )YYK ( A, A ) Y   e(e Y  ))   e  (   W e)
Wb
W
T

T T

W
(  )e
W
'

,



I
1 T
T
T T
 Y ( K ( A, A ) K ( A, A )  ee )Y
W
Wb

Decision function

 0,  Ai  G1
1 T
*
(( K ( Ai , A ) K ( A, A )  e )Y  ){
 0,  Ai  G2
Wb
T

2018/4/13

T T

BSC@YShi-Jan-2014

Big Data in Internet
 over 4.6 billion cell phone users
 > 2.5 billion internet users

2007 Telecommunication Data excceds 65 exabytes
2013 Internet Data will be 667 exabytes
Walmart has over 3 PetaBytes customer data
Facebook handles over 40 billion photos

Big Data is rapidly growing 
2018/4/13

BSC@YShi-Jan-2014

Big data Definition by National Science
Foundation, USA

"big data" refers to large, diverse,
complex, longitudinal, and/or
distributed data sets generated
from instruments, sensors,
Internet transactions, email, video,
click streams, and/or all other
digital sources available today
and in the future.
2018/4/13

BSC@YShi-Jan-2014

Evolution of Data Analysis
Traditional Data Analysis
From Data Collection, Statistical
Analysis to Descriptive Results
Data Mining Methods
From Data Collection, Data Preprocess,
Data Mining Algorithm to Knowledge
Discovery

What is Big Data Mining method
2018/4/13

BSC@YShi-Jan-2014

General Comments on Big Data Mining
·Not only all data, but also samples
because the larger samples can bring
better results than smaller ones
·Search for precise solutions from the
rough solutions;
·Find the reasoning based on
correlations.
* Big Data by Viktor, Cukier Mayer-schonberger, Houghton Mifflin Harcourt, 2013
2018/4/13

BSC@YShi-Jan-2014

Research on Big Data
Study the systematic characteristics of complexity
and uncertainty in Big Data.
 Explore the synthetics of "Man-Machine-Device" in
decision making process .
 Explore how to use optimization based data mining to
handle big data .
 Explore the evolution and process modes in
intelligent knowledge discovery in big data.
 Explore how to find intelligent knowledge under
cloud computing environment.
 Explore the representation, measurement, and
evaluation of intelligent knowledge in bid data.
 Explore the integration and visualization platform of
intelligent knowledge
under big data
2018/4/13
BSC@YShi-Jan-2014


Challenges to Management
Science

Data + Human Knowledge

= Decision Making
2018/4/13

BSC@YShi-Jan-2014

Challenge 1
Big Data
Rough
KnowledgeFirst ­ order mining
+ Human Knowledge
Intelligent Knowledge
Second-order mining
2018/4/13

BSC@YShi-Jan-2014

Challenge 2
Big Data pushes the change
of Management Science:
from traditional "modeldriven" and "case-driven" to
"data driven" decision
making
2018/4/13

BSC@YShi-Jan-2014

Example 1: Parallel Regularized
Multiple-Criteria Linear Programming

Zhiquan Qi, Vassil Alexandrov,, Yong Shi, Yingjie Tian, Parallel Regularized
Multiple-Criteria Linear Programming, Procedia Computer Science 2014.

Example 1: Parallel Regularized
Multiple-Criteria Linear Programming
·

Convex problem, through the dual:

·

Reformulate the problem into global optimization one

·

Parallelize efficiently:
- by dividing the variables
- by dividing the area to subareas

Zhiquan Qi, Vassil Alexandrov,, Yong Shi, Yingjie Tian, Parallel Regularized Multiple-Criteria
Linear Programming, tProcedia Computer Science 2014.

Example 1: Parallel RMCLP Classification Algorithm
and Its Application on the Medical Data

Zhiquan Qi, Vassil Alexandrov,, Yong Shi, Yingjie Tian, Parallel RMCLP Classification Algorithm and Its Application on
the Medical Data, IEEE Transactions on Cloud Computing, 2015.

Example 1: Parallel RMCLP Classification Algorithm
and Its Application on the Medical Data

.

Example2: Big Data-Credit Systems in China
· Since 2006, Credit Bureau of China Central Bank
has built the largest private credit database in
one nation
· 850 million personal records (more than 2/3 of
population in China)
· 45 million more visited daily
· From 2006-2009, CASFEDS developed credit
scoring models (called China Score) and
behavior prediction, which will serve as the basis
for the population of 1.3 billion in China for their
daily financial activities.
· It is my best contribution to China and can be
one of the most influential events to human
society.
2018/4/13
BSC@YShi-Jan-2014


100
92.87
90
80

Models
94.5

91.25
80.57

74.67

84.7
79.8
79.5

68.2

70

88.9

88.9

93.2

84.183.3 84

67.7

81.84
71.9

67.7

60
50
40
30
20
10
0

Logistic

MCLP

Good

Logistic
MCLP
MCQP
SVM
2018/4/13

Good
92.87
79.5
84.1
94.5

MCQP

Bad

Bad
74.67
84.7
83.3
71.9

Total

Total
91.25
79.8
84
93.2

SVM

KS

KS
68.2
67.7
67.7
69.63

BSC@YShi-Jan-2014

Gini

Gini
80.57
88.9
88.9
81.84

69.63

China Score vs. US Score vs.
Australia Score
US average Credit
Score is 678 out of
850:
· Excellent 750-850
· Good
660-749
· Fair
620-659
· Poor
350-619
Australia average Credit
Score is of 574

2018/4/13

Models

China
average

mcqp

674

mclp

657

svm

674

logit

681

BSC@YShi-Jan-2014

US Fico Score

2018/4/13

Poor

Fair
Good
BSC@YShi-Jan-2014

Excellent

Evaluations from Experts
Former Governor of PBC and The current
special advisor of International Monetary
Fund (IMF), Dr. Min Zhu said,
"CASFEDS research center has created the best
credit system for our Chinese which has been
far more accurate than any country's ones by
scientifically utilizing
our world largest
credit database."
2018/4/13

BSC@YShi-Jan-2014

Potential Impact
China Score will be used for
all of 1.3 billion Chinese to
handle daily commercial
life!
It will be one of the most
influential Big Data
applications in human life!!
2018/4/13

BSC@YShi-Jan-2014

Examples 3
Credit card analysis
Insurance Analysis

2018/4/13

BSC@YShi-Jan-2014

Books

2018/4/13

BSC@YShi-Jan-2014

