Assignment 4 - Parallel Processing
Nisim Hurst
Tuesday 27 February 2018
Abstract
Parallel processing has been found to be a the best alternative to circumvent Moore's law and come up
with energy efficient solutions. However, algorithms must be modified to identify potential improvements
and make the required modifications exclusively on the points that allow such parallelization. In this
assignment we explore the distinct libraries and choose some that can be used with Python without much
trouble.

Instructions
Part 1. Read the web page Parallel Processing and Multiprocessing in Python
https://wiki.python.org/moin/ParallelProcessing
a) Select one python parallel implementation in the case of Symmetric Multiprocessing (shared memory).
b) Select one python parallel implementation in the case of Cluster Computing (distributed memory).
Deliverable: Report.
In both cases, argue the technical reasons for your choice with respect to the other implementations. Make a
1-2 page analysis.

Part 2. Read the following posts:
1. Parallel K-means (https://github.com/utkarsh5k/parallel-kmeans)
2. Some generalities of MPI ( P. Pacheco, An Introduction to Parallel Programming, Morgan and
Kaufmann-Elsevier, 2011). and OpenCL (shared and distributed memory).
Deliverable: Report.
Discuss the performance's graphs showed in the post and results of clustering presented in the post. and
write down some comments about the obtained results. Make a 1-2 page analysis.

Deliverable Reports
Part 1. Technology selection
Symmetric Mutiprocessing Libraries Benchmark (shared memory)
Table 1: Symmetric Mutiprocessing Libraries Benchmark
Library

Pros

Cons

dispy

supports SMP
supports network

learning curve of the SIMD style

1

Library

Pros

Cons

delegate

fork based

pickle data only

forkmap

it resembles to Python's map function

forkfun

new version of forkmap

ppmap

variant of forkmap that manage
subprocesses
Simple assignment to shared containers

POSH
PyMP
pp (Parallel
Python)

contains all the functions of C, C++ and
Fortran that were included in OpenMP
process based
job oriented
OS agnostic

Unix only
For Python 3 is only available in Unix

PyMP is our selection
Local concurrency models can be familiar to the developer but limit their relevance to SMP hardware. These
approaches allow convenient process creation and shared memory and read only data structures. The chosen
library is PyMP which is a fork of OpenMP that supports Python 2 and 3 for Unix.
OpenMP characteristics
OpenMP is designed for a collection of cores all with access to shared memory. It allows the programmer
to implicitly indicate the compiler that some code block must be compiled in parallel and thus the thread
behavior is determined by the compiler. It was developed to make large scale high performance programs
easier to develop and to incrementally parallelize. However , some code level interactions are more difficult.
PyMP characteristics
PyMP supports minimal code changes (from C) to port it to Python and high efficiency. MP in OpenMP
stands for multiprocessing. It works by opening a parallel context where the processes can be forked depending
on the algorithm ending up with the same process state as the father process. However, the memory is
referenced and not directly copied until the process need to write information (and gets it's own copy if the
information).
Cluster Computing Libraries Benchmark (distributed memory)
Unlike thread-based concurrency that we saw in shared memory parallel computing, cluster architectures
offer higher scalability due to the absence of shared local resources. Through a network, the messages are
broadcasted to infinitely many nodes.

2

Table 2: Cluster Computing Libraries Benchmark
Library

Pros

Deap

Evolutionary ditributed task manager
Can be used separately to compute other
algorithms
Works with MPI
Works with PyMPI
Works with mpi4y
Works over TCP
Works with shared memory
SIMD style compatible
Nodes shareble to multiple processes
Efficient polling makes it high scalable
Works over SSH
Simple

Doesn't work in Python
2.7

IPython

Interactive notebooks

Can't be scheduled

pp (Parallel Python)

process based
job oriented
OS agnostic
MPI-based solution
Gentle learning curve
Standard over multiple technologies

dispy

DistributedPython

pyMPI

Cons

learning curve of the
SIMD style

PyMPI is our selection
PyMPI is extremely easy to use in python, as we will see in due course. Besides, it makes use of the flexible
MPI standard that is compatible with a wide variety of parallel computing architectures over C, C++ and
Fortran. According to Wikipedia about MPI:
. . . it has become a de facto standard for communication among processes that model a parallel
program running on a distributed memory system. . .
MPI has bindings for Java, Matlab, OCaml, Python and R.
MPI characteristics
In contrast to shared memory systems, MPI is a distributing memory system that uses message passing. A
distributed memory system exposes a network in which a the memory accessible to a core is only directly
accessible to that core (Pacheco 2011). MPI is an implementation in C, C++ and Fortran that stands
for Message Passing Interface and collects several libraries for that matter. In message passing programs,
the communication between processes occurs by calling functions, a function for send and another for
receiving. Important concepts on MPI are, one-sided communications, collective extensions, dynamic process
management and I/O management.
The most important technical characteristic of MPI is that uses Collective Tree-structured communications
vs classical point to point communications. We also have a new set of derived datatypes that will be more
cheap in the long run. So in each program we have to create that kind of structure. However, it also support
explicit shared memory programming since MPI-3.
PyMPI characteristics

3

PyMPI is a python interpreter that include a large subset of MPI functions. Example usage:
mpirun -np 3 pyMPI
> import mpi
> print "Hi, I'm process #%d" % mpi.rank
# Hi, I'm process #0
# Hi, I'm process #1
# Hi, I'm process #2
You can see from Listing 1 that is very straight forward to implement, the only thing that changes is that we
use another command line interpreter to run it, import mpi and then we can access the mpi.rank variable
which contains the process number.

Part 2. Parallel K-Means results discussion
In the following section first explore some of the characteristics of several parallel libraries, namely, MPI and
OpenCL. Then, we compare the time cost graph between them compared to the sequential algorithm as we
keep raising the cluster number. So, lets take a brief detour to comment the MPI and OpenCL characteristics.
Basic algorithm using MPI
As aforementioned MPI is a distributed memory model, so we can have trouble in the broadcast of memory.
This is the basic K-Means parallel algorithm:
1. Assign k points to p processors
2. In each processor chose k random points and calculate local means using point-to-point communications
3. Communicate means to the other processors using collective communications
4. Calculate membership function and return to step 1 until two optional stop conditions
1. The points don't change clusters between each processor
2. A threshold point of unclassified points is reached
3. Certain number of iterations was reached
5. In each node you can use OpenMP locally (thus you are still able to produce an hybrid approach and
keep the best of the two worlds).
OpenCL advantanges over MPI
OpenCL access the data in a shared memory mode (local from it's 4 hierarchical memory model) without
the need to send data through a local area network. It is also supported by big corporations like Apple, AMD,
IBM, Qualcomm, Intel, and Nvidia. It provides a standard interface for parallel computing using task and
data-based parallelism and can be compared to CUDA. It has a four memory hierarchy that consist on global,
read-only, local and per-element private memory. Devices can also share memory with the CPU. Consistency
between memory levels is implicitly relaxed and only enforced by the programmer, thus the learning curve is
greater. The general algorithm is as follows:
1.
2.
3.
4.
5.
6.

Node 0 chooses k points and assign them as cluster means (using tree-structured communication)
In each processor thread, for each point calculate the membership function over the cluster mean
Recalculate means in each clusters/processor
Find a global mean over all the clusters
Communicate to the shared memory the result of the centroids
Repeat until
1. The points don't change clusters between each processor

4

2. A threshold point of unclassified points is reached
3. Certain number of iterations was reached
Benchmark
The benchmark consist of 10000 samples each of 17 features on Github Parallel K-Means (Utkarsh5k 2018).
Now we proceed with the benchmark as we keep raising the cluster number:

Figure 1: Sequential performance over a large number of clusters

Figure 2: Sequential vs OpenCL vs MPI
Figure 1 shows an exponential growth complexity on the number of clusters using the sequential algorithm.
The graph on Figure 2 shows time taken between a sequential vs MPI vs OpenCL implementation with
respect with the number of clusters to make on the K-Means algorithm. We can see that since the sixth
cluster the improvement of OpenCL begins to supersede the improvement reached by MPI. Communication
seems to be taking a more important roll with so many clusters. Also, it seems that MPI membership function
allows it different cluster to retake the points other clusters have deleted.
Another point to notice is that MPI has an overhead on a greater number of clusters for message passing
between increasingly processes. On the contrary, OpenCL uses global memory and reduced the overhead of
5

communication.
Now lets see the quality of clusters produced by each of the algorithms:

Figure 3: Initial

Figure 4: Sequential
Figure 4 shows the cluster made by the sequential (slow) algorithm. The sequential algorithm doesn't split
the data. The sequential graph shows a more consistent clustering but detects only 3 clusters of the 4 required.
This phenomenon maybe due to a deficient centroid random initialization.

6

Figure 5: MPI
Figure 5 shows the cluster made by the MPI algorithm. MPI does a good job in generating 4 distinct clusters
clearly separated by a proportional distance over the x axis. The exclusion of other nodes when a centroid is
defined, is made known to the other processors through a broadcast process that may be less frequent but
more consistent than the communication that OpenCL makes over shared local memory.

Figure 6: OpenCL
Figure 6 shows the cluster made by the OpenCL algorithm. OpenCL may be taking shared data from the
shared CPU memory (that is very fast) but at the same time it can assume and give priority to local processor
data when there is doubt to which cluster does the point belongs. However, it fails to produce 4 distinct
clusters, overlapping the blue and yellow clusters.

Conclusions
1. In this homework we saw how there are many mature efficient parallel computing frameworks already
ported to Python like PyMPI and PyMP.
2. There are two main types of parallel computing: Symmetric Multiprocessing (local global shared
memory) and Cluster Computing (network based).
7

3. PyMP and OpenCL belongs to the Symmetric Multiprocessing category, while PyMPI belongs to
Cluster Computing.
4. Cluster Computing solutions can work along with local Symmetric Multiprocessing processing.
5. Symmetric Multiprocessing is convenient when a high number of clusters is needed and there is a high
CPU/GPU local availability.
6. Cluster Computing can be beneficial when we want to dynamically scale our solution to unknown
payloads and we have access to a node network.

References
Pacheco, Peter S. 2011. Introduction to Parallel Programming. doi:10.1007/978-1-4471-2736-9.
Utkarsh5k. 2018. "GitHub - utkarsh5k/parallel-kmeans: Parallel implementation of k-means clustering in
MPI and OpenCL." Accessed February 6. https://github.com/utkarsh5k/parallel-kmeans.

8

