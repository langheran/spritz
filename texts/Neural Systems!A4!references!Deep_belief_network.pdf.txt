Deep belief network
In machine learning, a deep belief network (DBN) is a generative graphical model,
or alternatively a class ofdeep neural network, composed of multiple layers of latent
variables ("hidden units"), with connections between the layers but not between
units within each layer.[1]
When trained on a set of examples without supervision, a DBN can learn to
probabilistically reconstruct its inputs. The layers then act as feature detectors.[1]
After this learning step, a DBN can be further trained with supervision to perform
classification.[2]
DBNs can be viewed as a composition of simple, unsupervised networks such as
restricted Boltzmann machines (RBMs)[1] or autoencoders,[3] where each subnetwork's hidden layer serves as the visible layer for the next. An RBM is an
undirected, generative energy-based model with a "visible" input layer and a hidden
layer and connections between but not within layers. This composition leads to a
fast, layer-by-layer unsupervised training procedure, where contrastive divergence is
applied to each sub-network in turn, starting from the "lowest" pair of layers (the
lowest visible layer is atraining set).
Teh's observation[2] that DBNs can be trained greedily, one layer at a time, led to
one of the first effective deep learning algorithms.[4]:6 Overall, there are many
attractive implementations and uses of DBNs in real-life applications and scenarios
(e.g., electroencephalography[5], drug discovery[6]).
Training
See also
References
External links
The training method for RBMs proposed by Geoffrey Hinton for use with training "Product of Expert" models is called contrastive
divergence (CD).[7] CD provides an approximation to the maximum likelihoodmethod that would ideally be applied for learning the
weights.[8][9] In training a single RBM, weight updates are performed with gradient descent via the following equation:
where, is the probability of a visible vector, which is given by . is the partition function (used for
normalizing) and is the energy function assigned to the state of the network. A lower energy indicates the network is in a
more "desirable" configuration. The gradient has the simple form where represent
Schematic overview of a deep belief
net. Arrows represent directed
connections in thegraphical model
that the net represents.
Contents
Trainingaverages with respect to distribution . The issue arises in sampling
because this requires extended alternating Gibbs sampling. CD replaces this step by
running alternating Gibbs sampling for steps (values of perform well).
After steps, the data are sampled and that sample is used in place of .
The CD procedure works as follows: [8]
1. Initialize the visible units to a training vector .
2. Update the hidden units in parallel given the visible units:
. is the sigmoid functionand is
the bias of .
3. Update the visible units in parallel given the hidden units:
. is the bias of . This is called
the "reconstruction" step.
4. Re-update the hidden units in parallel given the reconstructed visible
units using the same equation as in step 2.
5. Perform the weight update: .
Once an RBM is trained, another RBM is "stacked" atop it, taking its input from the
final trained layer. The new visible layer is initialized to a training vector, and values
for the units in the already-trained layers are assigned using the current weights and biases. The new RBM is then trained with the
procedure above. This whole process is repeated until the desired stopping criterion is met. [10]
Although the approximation of CD to maximum likelihood is crude (does not follow the gradient of any function), it is empirically
effective.[8]
Bayesian network
Deep learning
1. Hinton, G. (2009)."Deep belief networks"(http://www.scholarpedia.org/article/Deep_belief_networks). Scholarpedia.
4 (5): 5947. doi:10.4249/scholarpedia.5947(https://doi.org/10.4249%2Fscholarpedia.5947) .
2. Hinton, G. E.; Osindero, S.; Teh, Y. W. (2006). "A Fast Learning Algorithm for Deep Belief Nets"(http://www.cs.toront
o.edu/~hinton/absps/fastnc.pdf)(PDF). Neural Computation. 18 (7): 1527–1554.doi:10.1162/neco.2006.18.7.1527
(https://doi.org/10.1162%2Fneco.2006.18.7.1527) . PMID 16764513 (https://www.ncbi.nlm.nih.gov/pubmed/1676451
3).
3. Yoshua Bengio; Pascal Lamblin; Dan Popovi ci; Hugh Larochelle (2007).Greedy Layer-Wise Training of Deep
Networks (http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf) (PDF). NIPS.
4. Bengio, Y. (2009). "Learning Deep Architectures for AI"(http://www.iro.umontreal.ca/~lisa/pointeurs/ TR1312.pdf)
(PDF). Foundations and Trends in Machine Learning. 2. doi:10.1561/2200000006(https://doi.org/10.1561%2F22000
00006).
5. Movahedi, F.; Coyle, J. L.; Sejdic, E. (2017)."Deep belief networks for electroencephalography: A review of recent
contributions and future outlooks"(http://ieeexplore.ieee.org/abstract/document/7981315/) . IEEE Journal of
Biomedical and Health Informatics. PP (99): 1–1. doi:10.1109/JBHI.2017.2727218(https://doi.org/10.1109%2FJBHI.
2017.2727218). ISSN 2168-2194 (https://www.worldcat.org/issn/2168-2194).
6. Ghasemi, F.; Fassihi, A.; Perez-Sanchez, H.;Mehr idehnavi, A. (2017)."The role of different sampling methods in
improving biological activity prediction using deep belief network" (http://onlinelibrary.wiley.com/doi/10.1002/jcc.2467
1/full). Journal of Computational Chemistry. 38 (4): 195–203.doi:10.1002/jcc.24671(https://doi.org/10.1002%2Fjcc.2
4671).
A restricted Boltzmann machine
(RBM) with fully connected visible
and hidden units. Note there are no
hidden-hidden or visible-visible
connections.
See also
References"Deep Belief Networks". Deep Learning Tutorials.
"Deep Belief Network Example". Deeplearning4j Tutorials.
Retrieved from "https://en.wikipedia.org/w/index.php?title=Deep_belief_network&oldid=819277238 "
This page was last edited on 8 January 2018, at 13:25.
Text is available under theCreative Commons Attribution-ShareAlike License; additional terms may apply. By using this
site, you agree to theTerms of Use and Privacy Policy. Wikipedia® is a registered trademark of theWikimedia
Foundation, Inc., a non-profit organization.
7. G. E. Hinton.,"Training Product of Experts by Minimizing Contrastive Divergence,"(http://www.cs.toronto.edu/~fritz/a
bsps/nccd.pdf)Neural Computation, 14, pp. 1771–1800, 2002.
8. Hinton, G. E. (2010)."A Practical Guide to Training Restricted Boltzmann Machines"(https://www.researchgate.net/p
ublication/221166159_A_brief_introduction_to_Weightless_Neural_Systems). Tech. Rep. UTML TR 2010-003, .
9. Fischer, A.; Igel, C. (2014)."Training Restricted Boltzmann Machines: AnIntroduction"(http://image.diku.dk/igel/pap
er/TRBMAI.pdf)(PDF). Pattern Recognition. 47: 25–39. doi:10.1016/j.patcog.2013.05.025(https://doi.org/10.1016%
2Fj.patcog.2013.05.025).
10. Bengio, Yoshua (2009)."Learning Deep Architectures for AI"(http://sanghv.com/download/soft/machine%20learnin
g,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%2
0%282009%29.pdf)(PDF). Foundations and Trends in Machine Learning. 2 (1): 1–127. doi:10.1561/2200000006(htt
ps://doi.org/10.1561%2F2200000006) .
External links