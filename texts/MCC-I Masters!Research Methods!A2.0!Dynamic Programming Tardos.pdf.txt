Chapter 6
Dynamic Programming

We began our study of algorithmic techniques with greedy algorithms, which
in some sense form the most natural approach to algorithm design. Faced with
a new computational problem, we’ve seen that it’s not hard to propose multiple
possible greedy algorithms; the challenge is then to determine whether any of
these algorithms provides a correct solution to the problem in all cases.
The problems we saw in Chapter 4 were all uniﬁed by the fact that, in the
end, there really was a greedy algorithm that worked. Unfortunately, this is far
from being true in general; for most of the problems that one encounters, the
real difﬁculty is not in determining which of several greedy strategies is the
right one, but in the fact that there is no natural greedy algorithm that works.
For such problems, it is important to have other approaches at hand. Divide
and conquer can sometimes serve as an alternative approach, but the versions
of divide and conquer that we saw in the previous chapter are often not strong
enough to reduce exponential brute-force search down to polynomial time.
Rather, as we noted in Chapter 5, the applications there tended to reduce a
running time that was unnecessarily large, but already polynomial, down to a
faster running time.
We now turn to a more powerful and subtle design technique, dynamic
programming. It will be easier to say exactly what characterizes dynamic programming after we’ve seen it in action, but the basic idea is drawn from the
intuition behind divide and conquer and is essentially the opposite of the
greedy strategy: one implicitly explores the space of all possible solutions, by
carefully decomposing things into a series of subproblems, and then building up correct solutions to larger and larger subproblems. In a way, we can
thus view dynamic programming as operating dangerously close to the edge of

252

Chapter 6 Dynamic Programming

brute-force search: although it’s systematically working through the exponentially large set of possible solutions to the problem, it does this without ever
examining them all explicitly. It is because of this careful balancing act that
dynamic programming can be a tricky technique to get used to; it typically
takes a reasonable amount of practice before one is fully comfortable with it.
With this in mind, we now turn to a ﬁrst example of dynamic programming: the Weighted Interval Scheduling Problem that we deﬁned back in
Section 1.2. We are going to develop a dynamic programming algorithm for
this problem in two stages: ﬁrst as a recursive procedure that closely resembles
brute-force search; and then, by reinterpreting this procedure, as an iterative
algorithm that works by building up solutions to larger and larger subproblems.

6.1 Weighted Interval Scheduling:
A Recursive Procedure
We have seen that a particular greedy algorithm produces an optimal solution
to the Interval Scheduling Problem, where the goal is to accept as large a
set of nonoverlapping intervals as possible. The Weighted Interval Scheduling
Problem is a strictly more general version, in which each interval has a certain
value (or weight), and we want to accept a set of maximum value.

Designing a Recursive Algorithm
Since the original Interval Scheduling Problem is simply the special case in
which all values are equal to 1, we know already that most greedy algorithms
will not solve this problem optimally. But even the algorithm that worked
before (repeatedly choosing the interval that ends earliest) is no longer optimal
in this more general setting, as the simple example in Figure 6.1 shows.
Indeed, no natural greedy algorithm is known for this problem, which is
what motivates our switch to dynamic programming. As discussed above, we
will begin our introduction to dynamic programming with a recursive type of
algorithm for this problem, and then in the next section we’ll move to a more
iterative method that is closer to the style we use in the rest of this chapter.

Index
1
2
3

Value = 1
Value = 3
Value = 1

Figure 6.1 A simple instance of weighted interval scheduling.

